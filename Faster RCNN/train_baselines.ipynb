{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_baselines.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UauKNtFRqydu"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kauOILHSqXWw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30053bea-cfcc-4f8f-d816-c11f4ee37b81"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ujj5YcG5bIp"
      },
      "source": [
        "side_length=3\n",
        "phase=2#installing tf right version, or running alr\n",
        "\n",
        "step=1#which step of the iterative transfer learning: change to step 2 after first 80 epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fzxugFjqc2T"
      },
      "source": [
        "%pip install Tensorflow==2.3.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UauKNtFRqydu"
      },
      "source": [
        "### Import libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSbY3Amlqpkh"
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "import random\n",
        "import pprint\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "from optparse import OptionParser\n",
        "import pickle\n",
        "import math\n",
        "import cv2\n",
        "import copy\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "from keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout,concatenate\n",
        "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, TimeDistributed\n",
        "from keras.engine.topology import get_source_inputs\n",
        "from keras.utils import layer_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.objectives import categorical_crossentropy\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.utils import generic_utils\n",
        "from keras.engine import Layer, InputSpec\n",
        "from keras import initializers, regularizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "430F7HPMgj9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f4eaa06-e38a-45e5-c535-121c770107fe"
      },
      "source": [
        "#test file system\n",
        "\n",
        "f=open('/content/drive/My Drive/AI/Faster_RCNN/table_annotation_with_unlv.txt','r')\n",
        "lines=[l.rstrip() for l in f.readlines()]\n",
        "f.close()\n",
        "print(len(lines))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bneGu_87Q-aB"
      },
      "source": [
        "#### Plotting distribution of box sizes..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1F9G1BjQ7BL"
      },
      "source": [
        "# f=open('/content/drive/My Drive/AI/Faster_RCNN/table_annotation_with_unlv.txt','r')\n",
        "# lines=[l.rstrip() for l in f.readlines()]\n",
        "# f.close()\n",
        "# print(len(lines))\n",
        "\n",
        "# xx=[]\n",
        "# yy=[]\n",
        "\n",
        "# for i in range(len(lines)):\n",
        "#   if (i%5)==0:\n",
        "#     print(\"Completed {} images\".format(i))\n",
        "#   line=lines[i]\n",
        "#   data=line.split(',')\n",
        "#   height=int(data[4])-int(data[2])\n",
        "#   width=int(data[3])-int(data[1])\n",
        "\n",
        "#   #print(width,height) #x,y\n",
        "\n",
        "#   while True:\n",
        "#     try:\n",
        "#       k=Image.open(data[0]).size #x,y\n",
        "#       break\n",
        "#     except:\n",
        "#       print('Some weird error occured, lemme try again')\n",
        "#   k=min(k),min(k)\n",
        "\n",
        "#   xx.append(width/k[0])\n",
        "#   yy.append(height/k[1])\n",
        "\n",
        "#   xx.append(height/k[1])\n",
        "#   yy.append(width/k[0])\n",
        "\n",
        "# for i in range(len(xx)):\n",
        "#   xx[i]*=300\n",
        "#   yy[i]*=300\n",
        "# plt.plot(xx,yy,'o',markersize=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjKZ1ZiTWn_F"
      },
      "source": [
        "#plt.plot(xx,yy,'o',markersize=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrH5i5mmrDWY"
      },
      "source": [
        "#### Config setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvJm0FFRsyVu"
      },
      "source": [
        "class Config:\n",
        "\n",
        "\tdef __init__(self):\n",
        "\n",
        "\t\t# Print the process or not\n",
        "\t\tself.verbose = True\n",
        "\n",
        "\t\t# Name of base network\n",
        "\t\tself.network = 'vgg'\n",
        "\n",
        "\t\t# Setting for data augmentation\n",
        "\t\tself.use_horizontal_flips = False\n",
        "\t\tself.use_vertical_flips = False\n",
        "\t\tself.rot_90 = False\n",
        "\n",
        "\t\t# Anchor box scales\n",
        "    # Note that if im_size is smaller, anchor_box_scales should be scaled\n",
        "    # Original anchor_box_scales in the paper is [128, 256, 512]\n",
        "\t\tself.anchor_box_scales = [64, 128, 256] \n",
        "\n",
        "\t\t# Anchor box ratios\n",
        "\t\t#self.anchor_box_ratios = [[1, 1], [1./math.sqrt(2), 2./math.sqrt(2)], [2./math.sqrt(2), 1./math.sqrt(2)]]#from the original download\n",
        "\t\tself.anchor_box_ratios = [[1, 1], [1 , 1.35], [1.35 , 1]]\n",
        "\n",
        "\t\t# Size to resize the smallest side of the image\n",
        "\t\t# Original setting in paper is 600. Set to 300 in here to save training time\n",
        "\t\tself.im_size = 300\n",
        "\n",
        "\t\t# image channel-wise mean to subtract\n",
        "\t\tself.img_channel_mean = [103.939, 116.779, 123.68]\n",
        "\t\tself.img_scaling_factor = 1.0\n",
        "\n",
        "\t\t# number of ROIs at once\n",
        "\t\tself.num_rois = 4\n",
        "\n",
        "\t\t# stride at the RPN (this depends on the network configuration)\n",
        "\t\tself.rpn_stride = 16\n",
        "\n",
        "\t\tself.balanced_classes = False\n",
        "\n",
        "\t\t# scaling the stdev\n",
        "\t\tself.std_scaling = 4.0\n",
        "\t\tself.classifier_regr_std = [8.0, 8.0, 4.0, 4.0]\n",
        "\n",
        "\t\t# overlaps for RPN\n",
        "\t\tself.rpn_min_overlap = 0.3\n",
        "\t\tself.rpn_max_overlap = 0.7\n",
        "\n",
        "\t\t# overlaps for classifier ROIs\n",
        "\t\tself.classifier_min_overlap = 0.1\n",
        "\t\tself.classifier_max_overlap = 0.5\n",
        "\n",
        "\t\t# placeholder for the class mapping, automatically generated by the parser\n",
        "\t\tself.class_mapping = None\n",
        "\n",
        "\t\tself.model_path = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0bIjlycyR9_"
      },
      "source": [
        "#### Parser the data from annotation file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc89E9uAydTX"
      },
      "source": [
        "def get_data(input_path):\n",
        "\t\"\"\"Parse the data from annotation file\n",
        "\t\n",
        "\tArgs:\n",
        "\t\tinput_path: annotation file path\n",
        "\n",
        "\tReturns:\n",
        "\t\tall_data: list(filepath, width, height, list(bboxes))\n",
        "\t\tclasses_count: dict{key:class_name, value:count_num} \n",
        "\t\t\te.g. {'Car': 2383, 'Mobile phone': 1108, 'Person': 3745}\n",
        "\t\tclass_mapping: dict{key:class_name, value: idx}\n",
        "\t\t\te.g. {'Car': 0, 'Mobile phone': 1, 'Person': 2}\n",
        "\t\"\"\"\n",
        "\tfound_bg = False\n",
        "\tall_imgs = {}\n",
        "\n",
        "\tclasses_count = {}\n",
        "\n",
        "\tclass_mapping = {}\n",
        "\n",
        "\tvisualise = True\n",
        "\n",
        "\ti = 1\n",
        "\t\n",
        "\twith open(input_path,'r') as f:\n",
        "\n",
        "\t\tprint('Parsing annotation files')\n",
        "\n",
        "\t\tfor line in f:\n",
        "\n",
        "\t\t\t# Print process\n",
        "\t\t\tsys.stdout.write('\\r'+'idx=' + str(i))\n",
        "\t\t\ti += 1\n",
        "\n",
        "\t\t\tline_split = line.strip().split(',')\n",
        "\n",
        "\t\t\t# Make sure the info saved in annotation file matching the format (path_filename, x1, y1, x2, y2, class_name)\n",
        "\t\t\t# Note:\n",
        "\t\t\t#\tOne path_filename might has several classes (class_name)\n",
        "\t\t\t#\tx1, y1, x2, y2 are the pixel value of the origial image, not the ratio value\n",
        "\t\t\t#\t(x1, y1) top left coordinates; (x2, y2) bottom right coordinates\n",
        "\t\t\t#   x1,y1-------------------\n",
        "\t\t\t#\t|\t\t\t\t\t\t|\n",
        "\t\t\t#\t|\t\t\t\t\t\t|\n",
        "\t\t\t#\t|\t\t\t\t\t\t|\n",
        "\t\t\t#\t|\t\t\t\t\t\t|\n",
        "\t\t\t#\t---------------------x2,y2\n",
        "\n",
        "\t\t\t(filename,x1,y1,x2,y2,class_name) = line_split\n",
        "\n",
        "\t\t\tif class_name not in classes_count:\n",
        "\t\t\t\tclasses_count[class_name] = 1\n",
        "\t\t\telse:\n",
        "\t\t\t\tclasses_count[class_name] += 1\n",
        "\n",
        "\t\t\tif class_name not in class_mapping:\n",
        "\t\t\t\tif class_name == 'bg' and found_bg == False:\n",
        "\t\t\t\t\tprint('Found class name with special name bg. Will be treated as a background region (this is usually for hard negative mining).')\n",
        "\t\t\t\t\tfound_bg = True\n",
        "\t\t\t\tclass_mapping[class_name] = len(class_mapping)\n",
        "\n",
        "\t\t\tif filename not in all_imgs:\n",
        "\t\t\t\tall_imgs[filename] = {}\n",
        "\t\t\t\t\n",
        "\t\t\t\timg = cv2.imread(filename)\n",
        "\t\t\t\t(rows,cols) = img.shape[:2]\n",
        "\t\t\t\tall_imgs[filename]['filepath'] = filename\n",
        "\t\t\t\tall_imgs[filename]['width'] = cols\n",
        "\t\t\t\tall_imgs[filename]['height'] = rows\n",
        "\t\t\t\tall_imgs[filename]['bboxes'] = []\n",
        "\t\t\t\t# if np.random.randint(0,6) > 0:\n",
        "\t\t\t\t# \tall_imgs[filename]['imageset'] = 'trainval'\n",
        "\t\t\t\t# else:\n",
        "\t\t\t\t# \tall_imgs[filename]['imageset'] = 'test'\n",
        "\n",
        "\t\t\tall_imgs[filename]['bboxes'].append({'class': class_name, 'x1': int(x1), 'x2': int(x2), 'y1': int(y1), 'y2': int(y2)})\n",
        "\n",
        "\n",
        "\t\tall_data = []\n",
        "\t\tfor key in all_imgs:\n",
        "\t\t\tall_data.append(all_imgs[key])\n",
        "\t\t\n",
        "\t\t# make sure the bg class is last in the list\n",
        "\t\tif found_bg:\n",
        "\t\t\tif class_mapping['bg'] != len(class_mapping) - 1:\n",
        "\t\t\t\tkey_to_switch = [key for key in class_mapping.keys() if class_mapping[key] == len(class_mapping)-1][0]\n",
        "\t\t\t\tval_to_switch = class_mapping['bg']\n",
        "\t\t\t\tclass_mapping['bg'] = len(class_mapping) - 1\n",
        "\t\t\t\tclass_mapping[key_to_switch] = val_to_switch\n",
        "\t\t\n",
        "\t\treturn all_data, classes_count, class_mapping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFvqGs4acGWl"
      },
      "source": [
        "#### Define ROI Pooling Convolutional Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l32Q85kcMpB"
      },
      "source": [
        "class RoiPoolingConv(Layer):\n",
        "    '''ROI pooling layer for 2D inputs.\n",
        "    See Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,\n",
        "    K. He, X. Zhang, S. Ren, J. Sun\n",
        "    # Arguments\n",
        "        pool_size: int\n",
        "            Size of pooling region to use. pool_size = 7 will result in a 7x7 region.\n",
        "        num_rois: number of regions of interest to be used\n",
        "    # Input shape\n",
        "        list of two 4D tensors [X_img,X_roi] with shape:\n",
        "        X_img:\n",
        "        `(1, rows, cols, channels)`\n",
        "        X_roi:\n",
        "        `(1,num_rois,4)` list of rois, with ordering (x,y,w,h)\n",
        "    # Output shape\n",
        "        3D tensor with shape:\n",
        "        `(1, num_rois, channels, pool_size, pool_size)`\n",
        "    '''\n",
        "    def __init__(self, pool_size, num_rois, **kwargs):\n",
        "\n",
        "        self.dim_ordering = K.image_data_format()\n",
        "        self.pool_size = pool_size\n",
        "        self.num_rois = num_rois\n",
        "\n",
        "        super(RoiPoolingConv, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.nb_channels = input_shape[0][3]   \n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "\n",
        "        assert(len(x) == 2)\n",
        "\n",
        "        # x[0] is image with shape (rows, cols, channels)\n",
        "        img = x[0]\n",
        "\n",
        "        # x[1] is roi with shape (num_rois,4) with ordering (x,y,w,h)\n",
        "        rois = x[1]\n",
        "\n",
        "        input_shape = K.shape(img)\n",
        "\n",
        "        outputs = []\n",
        "\n",
        "        for roi_idx in range(self.num_rois):\n",
        "\n",
        "            x = rois[0, roi_idx, 0]\n",
        "            y = rois[0, roi_idx, 1]\n",
        "            w = rois[0, roi_idx, 2]\n",
        "            h = rois[0, roi_idx, 3]\n",
        "\n",
        "            x = K.cast(x, 'int32')\n",
        "            y = K.cast(y, 'int32')\n",
        "            w = K.cast(w, 'int32')\n",
        "            h = K.cast(h, 'int32')\n",
        "\n",
        "            # Resized roi of the image to pooling size (7x7)\n",
        "            rs = tf.image.resize(img[:, y:y+h, x:x+w, :], (self.pool_size, self.pool_size))\n",
        "            outputs.append(rs)\n",
        "                \n",
        "\n",
        "        final_output = K.concatenate(outputs, axis=0)\n",
        "\n",
        "        # Reshape to (1, num_rois, pool_size, pool_size, nb_channels)\n",
        "        # Might be (1, 4, 7, 7, 3)\n",
        "        final_output = K.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))\n",
        "\n",
        "        # permute_dimensions is similar to transpose\n",
        "        final_output = K.permute_dimensions(final_output, (0, 1, 2, 3, 4))\n",
        "\n",
        "        return final_output\n",
        "    \n",
        "    \n",
        "    def get_config(self):\n",
        "        config = {'pool_size': self.pool_size,\n",
        "                  'num_rois': self.num_rois}\n",
        "        base_config = super(RoiPoolingConv, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf2taA29RFNs"
      },
      "source": [
        "#### Vgg-16 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaBQfl4XRJY3"
      },
      "source": [
        "def get_img_output_length(width, height):\n",
        "    def get_output_length(input_length):\n",
        "        return input_length//16\n",
        "\n",
        "    return get_output_length(width), get_output_length(height)    \n",
        "\n",
        "def nn_base(input_tensor=None, trainable=False):\n",
        "\n",
        "\n",
        "    input_shape = (None, None, 3)\n",
        "\n",
        "    if input_tensor is None:\n",
        "        img_input = Input(shape=input_shape)\n",
        "    else:\n",
        "        if not K.is_keras_tensor(input_tensor):\n",
        "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
        "        else:\n",
        "            img_input = input_tensor\n",
        "\n",
        "    bn_axis = 3\n",
        "\n",
        "    # Block 1\n",
        "    global side_length\n",
        "    if side_length==3:\n",
        "      x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
        "      x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
        "    else:\n",
        "      x1= Conv2D(32, (3, side_length), activation='relu', padding='same', name='block1_conv1a')(img_input)\n",
        "      x2= Conv2D(32, (side_length, 3), activation='relu', padding='same', name='block1_conv1b')(img_input)\n",
        "      x=concatenate([x1,x2])\n",
        "      x1= Conv2D(32, (3, side_length), activation='relu', padding='same', name='block1_conv2a')(x)\n",
        "      x2= Conv2D(32, (side_length, 3), activation='relu', padding='same', name='block1_conv2b')(x)\n",
        "      x=concatenate([x1,x2])\n",
        "\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
        "\n",
        "    # Block 2\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
        "\n",
        "    # Block 3\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
        "\n",
        "    # Block 4\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "\n",
        "    # Block 5\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
        "    # x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcOi5MIMVJpU"
      },
      "source": [
        "####  RPN layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsuV21vpRczQ"
      },
      "source": [
        "def rpn_layer(base_layers, num_anchors):\n",
        "    \"\"\"Create a rpn layer\n",
        "        Step1: Pass through the feature map from base layer to a 3x3 512 channels convolutional layer\n",
        "                Keep the padding 'same' to preserve the feature map's size\n",
        "        Step2: Pass the step1 to two (1,1) convolutional layer to replace the fully connected layer\n",
        "                classification layer: num_anchors (9 in here) channels for 0, 1 sigmoid activation output\n",
        "                regression layer: num_anchors*4 (36 in here) channels for computing the regression of bboxes with linear activation\n",
        "    Args:\n",
        "        base_layers: vgg in here\n",
        "        num_anchors: 9 in here\n",
        "\n",
        "    Returns:\n",
        "        [x_class, x_regr, base_layers]\n",
        "        x_class: classification for whether it's an object\n",
        "        x_regr: bboxes regression\n",
        "        base_layers: vgg in here\n",
        "    \"\"\"\n",
        "    x = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(base_layers)\n",
        "\n",
        "    x_class = Conv2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)\n",
        "    x_regr = Conv2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)\n",
        "\n",
        "    return [x_class, x_regr, base_layers]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fBt9xNFWsKS"
      },
      "source": [
        "####  Classifier layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PKSPLRLWwMz"
      },
      "source": [
        "def classifier_layer(base_layers, input_rois, num_rois, nb_classes = 4):\n",
        "    \"\"\"Create a classifier layer\n",
        "    \n",
        "    Args:\n",
        "        base_layers: vgg\n",
        "        input_rois: `(1,num_rois,4)` list of rois, with ordering (x,y,w,h)\n",
        "        num_rois: number of rois to be processed in one time (4 in here)\n",
        "\n",
        "    Returns:\n",
        "        list(out_class, out_regr)\n",
        "        out_class: classifier layer output\n",
        "        out_regr: regression layer output\n",
        "    \"\"\"\n",
        "\n",
        "    input_shape = (num_rois,7,7,512)\n",
        "\n",
        "    pooling_regions = 7\n",
        "\n",
        "    # out_roi_pool.shape = (1, num_rois, channels, pool_size, pool_size)\n",
        "    # num_rois (4) 7x7 roi pooling\n",
        "    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])\n",
        "\n",
        "    # Flatten the convlutional layer and connected to 2 FC and 2 dropout\n",
        "    out = TimeDistributed(Flatten(name='flatten'))(out_roi_pool)\n",
        "    out = TimeDistributed(Dense(4096, activation='relu', name='fc1'))(out)\n",
        "    out = TimeDistributed(Dropout(0.5))(out)\n",
        "    out = TimeDistributed(Dense(4096, activation='relu', name='fc2'))(out)\n",
        "    out = TimeDistributed(Dropout(0.5))(out)\n",
        "\n",
        "    # There are two output layer\n",
        "    # out_class: softmax acivation function for classify the class name of the object\n",
        "    # out_regr: linear activation function for bboxes coordinates regression\n",
        "    out_class = TimeDistributed(Dense(nb_classes, activation='softmax', kernel_initializer='zero'), name='dense_class_{}'.format(nb_classes))(out)\n",
        "    # note: no regression target for bg class\n",
        "    out_regr = TimeDistributed(Dense(4 * (nb_classes-1), activation='linear', kernel_initializer='zero'), name='dense_regress_{}'.format(nb_classes))(out)\n",
        "\n",
        "    return [out_class, out_regr]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMev3UMadCzJ"
      },
      "source": [
        "#### Calculate IoU (Intersection of Union)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy5iIBYgdCJD"
      },
      "source": [
        "def union(au, bu, area_intersection):\n",
        "\tarea_a = (au[2] - au[0]) * (au[3] - au[1])\n",
        "\tarea_b = (bu[2] - bu[0]) * (bu[3] - bu[1])\n",
        "\tarea_union = area_a + area_b - area_intersection\n",
        "\treturn area_union\n",
        "\n",
        "\n",
        "def intersection(ai, bi):\n",
        "\tx = max(ai[0], bi[0])\n",
        "\ty = max(ai[1], bi[1])\n",
        "\tw = min(ai[2], bi[2]) - x\n",
        "\th = min(ai[3], bi[3]) - y\n",
        "\tif w < 0 or h < 0:\n",
        "\t\treturn 0\n",
        "\treturn w*h\n",
        "\n",
        "\n",
        "def iou(a, b):\n",
        "\t# a and b should be (x1,y1,x2,y2)\n",
        "\n",
        "\tif a[0] >= a[2] or a[1] >= a[3] or b[0] >= b[2] or b[1] >= b[3]:\n",
        "\t\treturn 0.0\n",
        "\n",
        "\tarea_i = intersection(a, b)\n",
        "\tarea_u = union(a, b, area_i)\n",
        "\n",
        "\treturn float(area_i) / float(area_u + 1e-6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcRlzqZudKkd"
      },
      "source": [
        "#### Calculate the rpn for all anchors of all images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daPsCZtrdK3S"
      },
      "source": [
        "def calc_rpn(C, img_data, width, height, resized_width, resized_height, img_length_calc_function):\n",
        "\t\"\"\"(Important part!) Calculate the rpn for all anchors \n",
        "\t\tIf feature map has shape 38x50=1900, there are 1900x9=17100 potential anchors\n",
        "\t\n",
        "\tArgs:\n",
        "\t\tC: config\n",
        "\t\timg_data: augmented image data\n",
        "\t\twidth: original image width (e.g. 600)\n",
        "\t\theight: original image height (e.g. 800)\n",
        "\t\tresized_width: resized image width according to C.im_size (e.g. 300)\n",
        "\t\tresized_height: resized image height according to C.im_size (e.g. 400)\n",
        "\t\timg_length_calc_function: function to calculate final layer's feature map (of base model) size according to input image size\n",
        "\n",
        "\tReturns:\n",
        "\t\ty_rpn_cls: list(num_bboxes, y_is_box_valid + y_rpn_overlap)\n",
        "\t\t\ty_is_box_valid: 0 or 1 (0 means the box is invalid, 1 means the box is valid)\n",
        "\t\t\ty_rpn_overlap: 0 or 1 (0 means the box is not an object, 1 means the box is an object)\n",
        "\t\ty_rpn_regr: list(num_bboxes, 4*y_rpn_overlap + y_rpn_regr)\n",
        "\t\t\ty_rpn_regr: x1,y1,x2,y2 bunding boxes coordinates\n",
        "\t\"\"\"\n",
        "\tdownscale = float(C.rpn_stride) \n",
        "\tanchor_sizes = C.anchor_box_scales   # 128, 256, 512\n",
        "\tanchor_ratios = C.anchor_box_ratios  # 1:1, 1:2*sqrt(2), 2*sqrt(2):1\n",
        "\tnum_anchors = len(anchor_sizes) * len(anchor_ratios) # 3x3=9\n",
        "\n",
        "\t# calculate the output map size based on the network architecture\n",
        "\t(output_width, output_height) = img_length_calc_function(resized_width, resized_height)\n",
        "\n",
        "\tn_anchratios = len(anchor_ratios)    # 3\n",
        "\t\n",
        "\t# initialise empty output objectives\n",
        "\ty_rpn_overlap = np.zeros((output_height, output_width, num_anchors))\n",
        "\ty_is_box_valid = np.zeros((output_height, output_width, num_anchors))\n",
        "\ty_rpn_regr = np.zeros((output_height, output_width, num_anchors * 4))\n",
        "\n",
        "\tnum_bboxes = len(img_data['bboxes'])\n",
        "\n",
        "\tnum_anchors_for_bbox = np.zeros(num_bboxes).astype(int)\n",
        "\tbest_anchor_for_bbox = -1*np.ones((num_bboxes, 4)).astype(int)\n",
        "\tbest_iou_for_bbox = np.zeros(num_bboxes).astype(np.float32)\n",
        "\tbest_x_for_bbox = np.zeros((num_bboxes, 4)).astype(int)\n",
        "\tbest_dx_for_bbox = np.zeros((num_bboxes, 4)).astype(np.float32)\n",
        "\n",
        "\t# get the GT box coordinates, and resize to account for image resizing\n",
        "\tgta = np.zeros((num_bboxes, 4))\n",
        "\tfor bbox_num, bbox in enumerate(img_data['bboxes']):\n",
        "\t\t# get the GT box coordinates, and resize to account for image resizing\n",
        "\t\tgta[bbox_num, 0] = bbox['x1'] * (resized_width / float(width))\n",
        "\t\tgta[bbox_num, 1] = bbox['x2'] * (resized_width / float(width))\n",
        "\t\tgta[bbox_num, 2] = bbox['y1'] * (resized_height / float(height))\n",
        "\t\tgta[bbox_num, 3] = bbox['y2'] * (resized_height / float(height))\n",
        "\t\n",
        "\t# rpn ground truth\n",
        "\n",
        "\tfor anchor_size_idx in range(len(anchor_sizes)):\n",
        "\t\tfor anchor_ratio_idx in range(n_anchratios):\n",
        "\t\t\tanchor_x = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][0]\n",
        "\t\t\tanchor_y = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][1]\t\n",
        "\t\t\t\n",
        "\t\t\tfor ix in range(output_width):\t\t\t\t\t\n",
        "\t\t\t\t# x-coordinates of the current anchor box\t\n",
        "\t\t\t\tx1_anc = downscale * (ix + 0.5) - anchor_x / 2\n",
        "\t\t\t\tx2_anc = downscale * (ix + 0.5) + anchor_x / 2\t\n",
        "\t\t\t\t\n",
        "\t\t\t\t# ignore boxes that go across image boundaries\t\t\t\t\t\n",
        "\t\t\t\tif x1_anc < 0 or x2_anc > resized_width:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\t\t\n",
        "\t\t\t\tfor jy in range(output_height):\n",
        "\n",
        "\t\t\t\t\t# y-coordinates of the current anchor box\n",
        "\t\t\t\t\ty1_anc = downscale * (jy + 0.5) - anchor_y / 2\n",
        "\t\t\t\t\ty2_anc = downscale * (jy + 0.5) + anchor_y / 2\n",
        "\n",
        "\t\t\t\t\t# ignore boxes that go across image boundaries\n",
        "\t\t\t\t\tif y1_anc < 0 or y2_anc > resized_height:\n",
        "\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t# bbox_type indicates whether an anchor should be a target\n",
        "\t\t\t\t\t# Initialize with 'negative'\n",
        "\t\t\t\t\tbbox_type = 'neg'\n",
        "\n",
        "\t\t\t\t\t# this is the best IOU for the (x,y) coord and the current anchor\n",
        "\t\t\t\t\t# note that this is different from the best IOU for a GT bbox\n",
        "\t\t\t\t\tbest_iou_for_loc = 0.0\n",
        "\n",
        "\t\t\t\t\tfor bbox_num in range(num_bboxes):\n",
        "\t\t\t\t\t\t\n",
        "\t\t\t\t\t\t# get IOU of the current GT box and the current anchor box\n",
        "\t\t\t\t\t\tcurr_iou = iou([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]], [x1_anc, y1_anc, x2_anc, y2_anc])\n",
        "\t\t\t\t\t\t# calculate the regression targets if they will be needed\n",
        "\t\t\t\t\t\tif curr_iou > best_iou_for_bbox[bbox_num] or curr_iou > C.rpn_max_overlap:\n",
        "\t\t\t\t\t\t\tcx = (gta[bbox_num, 0] + gta[bbox_num, 1]) / 2.0\n",
        "\t\t\t\t\t\t\tcy = (gta[bbox_num, 2] + gta[bbox_num, 3]) / 2.0\n",
        "\t\t\t\t\t\t\tcxa = (x1_anc + x2_anc)/2.0\n",
        "\t\t\t\t\t\t\tcya = (y1_anc + y2_anc)/2.0\n",
        "\n",
        "\t\t\t\t\t\t\t# x,y are the center point of ground-truth bbox\n",
        "\t\t\t\t\t\t\t# xa,ya are the center point of anchor bbox (xa=downscale * (ix + 0.5); ya=downscale * (iy+0.5))\n",
        "\t\t\t\t\t\t\t# w,h are the width and height of ground-truth bbox\n",
        "\t\t\t\t\t\t\t# wa,ha are the width and height of anchor bboxe\n",
        "\t\t\t\t\t\t\t# tx = (x - xa) / wa\n",
        "\t\t\t\t\t\t\t# ty = (y - ya) / ha\n",
        "\t\t\t\t\t\t\t# tw = log(w / wa)\n",
        "\t\t\t\t\t\t\t# th = log(h / ha)\n",
        "\t\t\t\t\t\t\ttx = (cx - cxa) / (x2_anc - x1_anc)\n",
        "\t\t\t\t\t\t\tty = (cy - cya) / (y2_anc - y1_anc)\n",
        "\t\t\t\t\t\t\ttw = np.log((gta[bbox_num, 1] - gta[bbox_num, 0]) / (x2_anc - x1_anc))\n",
        "\t\t\t\t\t\t\tth = np.log((gta[bbox_num, 3] - gta[bbox_num, 2]) / (y2_anc - y1_anc))\n",
        "\t\t\t\t\t\t\n",
        "\t\t\t\t\t\tif img_data['bboxes'][bbox_num]['class'] != 'bg':\n",
        "\n",
        "\t\t\t\t\t\t\t# all GT boxes should be mapped to an anchor box, so we keep track of which anchor box was best\n",
        "\t\t\t\t\t\t\tif curr_iou > best_iou_for_bbox[bbox_num]:\n",
        "\t\t\t\t\t\t\t\tbest_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]\n",
        "\t\t\t\t\t\t\t\tbest_iou_for_bbox[bbox_num] = curr_iou\n",
        "\t\t\t\t\t\t\t\tbest_x_for_bbox[bbox_num,:] = [x1_anc, x2_anc, y1_anc, y2_anc]\n",
        "\t\t\t\t\t\t\t\tbest_dx_for_bbox[bbox_num,:] = [tx, ty, tw, th]\n",
        "\n",
        "\t\t\t\t\t\t\t# we set the anchor to positive if the IOU is >0.7 (it does not matter if there was another better box, it just indicates overlap)\n",
        "\t\t\t\t\t\t\tif curr_iou > C.rpn_max_overlap:\n",
        "\t\t\t\t\t\t\t\tbbox_type = 'pos'\n",
        "\t\t\t\t\t\t\t\tnum_anchors_for_bbox[bbox_num] += 1\n",
        "\t\t\t\t\t\t\t\t# we update the regression layer target if this IOU is the best for the current (x,y) and anchor position\n",
        "\t\t\t\t\t\t\t\tif curr_iou > best_iou_for_loc:\n",
        "\t\t\t\t\t\t\t\t\tbest_iou_for_loc = curr_iou\n",
        "\t\t\t\t\t\t\t\t\tbest_regr = (tx, ty, tw, th)\n",
        "\n",
        "\t\t\t\t\t\t\t# if the IOU is >0.3 and <0.7, it is ambiguous and no included in the objective\n",
        "\t\t\t\t\t\t\tif C.rpn_min_overlap < curr_iou < C.rpn_max_overlap:\n",
        "\t\t\t\t\t\t\t\t# gray zone between neg and pos\n",
        "\t\t\t\t\t\t\t\tif bbox_type != 'pos':\n",
        "\t\t\t\t\t\t\t\t\tbbox_type = 'neutral'\n",
        "\n",
        "\t\t\t\t\t# turn on or off outputs depending on IOUs\n",
        "\t\t\t\t\tif bbox_type == 'neg':\n",
        "\t\t\t\t\t\ty_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n",
        "\t\t\t\t\t\ty_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n",
        "\t\t\t\t\telif bbox_type == 'neutral':\n",
        "\t\t\t\t\t\ty_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n",
        "\t\t\t\t\t\ty_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n",
        "\t\t\t\t\telif bbox_type == 'pos':\n",
        "\t\t\t\t\t\ty_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n",
        "\t\t\t\t\t\ty_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n",
        "\t\t\t\t\t\tstart = 4 * (anchor_ratio_idx + n_anchratios * anchor_size_idx)\n",
        "\t\t\t\t\t\ty_rpn_regr[jy, ix, start:start+4] = best_regr\n",
        "\n",
        "\t# we ensure that every bbox has at least one positive RPN region\n",
        "\n",
        "\tfor idx in range(num_anchors_for_bbox.shape[0]):\n",
        "\t\tif num_anchors_for_bbox[idx] == 0:\n",
        "\t\t\t# no box with an IOU greater than zero ...\n",
        "\t\t\tif best_anchor_for_bbox[idx, 0] == -1:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\ty_is_box_valid[\n",
        "\t\t\t\tbest_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *\n",
        "\t\t\t\tbest_anchor_for_bbox[idx,3]] = 1\n",
        "\t\t\ty_rpn_overlap[\n",
        "\t\t\t\tbest_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *\n",
        "\t\t\t\tbest_anchor_for_bbox[idx,3]] = 1\n",
        "\t\t\tstart = 4 * (best_anchor_for_bbox[idx,2] + n_anchratios * best_anchor_for_bbox[idx,3])\n",
        "\t\t\ty_rpn_regr[\n",
        "\t\t\t\tbest_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], start:start+4] = best_dx_for_bbox[idx, :]\n",
        "\n",
        "\ty_rpn_overlap = np.transpose(y_rpn_overlap, (2, 0, 1))\n",
        "\ty_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=0)\n",
        "\n",
        "\ty_is_box_valid = np.transpose(y_is_box_valid, (2, 0, 1))\n",
        "\ty_is_box_valid = np.expand_dims(y_is_box_valid, axis=0)\n",
        "\n",
        "\ty_rpn_regr = np.transpose(y_rpn_regr, (2, 0, 1))\n",
        "\ty_rpn_regr = np.expand_dims(y_rpn_regr, axis=0)\n",
        "\n",
        "\tpos_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 1, y_is_box_valid[0, :, :, :] == 1))\n",
        "\tneg_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 0, y_is_box_valid[0, :, :, :] == 1))\n",
        "\n",
        "\tnum_pos = len(pos_locs[0])\n",
        "\n",
        "\t# one issue is that the RPN has many more negative than positive regions, so we turn off some of the negative\n",
        "\t# regions. We also limit it to 256 regions.\n",
        "\tnum_regions = 256\n",
        "\n",
        "\tif len(pos_locs[0]) > num_regions/2:\n",
        "\t\tval_locs = random.sample(range(len(pos_locs[0])), len(pos_locs[0]) - num_regions/2)\n",
        "\t\ty_is_box_valid[0, pos_locs[0][val_locs], pos_locs[1][val_locs], pos_locs[2][val_locs]] = 0\n",
        "\t\tnum_pos = num_regions/2\n",
        "\n",
        "\tif len(neg_locs[0]) + num_pos > num_regions:\n",
        "\t\tval_locs = random.sample(range(len(neg_locs[0])), len(neg_locs[0]) - num_pos)\n",
        "\t\ty_is_box_valid[0, neg_locs[0][val_locs], neg_locs[1][val_locs], neg_locs[2][val_locs]] = 0\n",
        "\n",
        "\ty_rpn_cls = np.concatenate([y_is_box_valid, y_rpn_overlap], axis=1)\n",
        "\ty_rpn_regr = np.concatenate([np.repeat(y_rpn_overlap, 4, axis=1), y_rpn_regr], axis=1)\n",
        "\n",
        "\treturn np.copy(y_rpn_cls), np.copy(y_rpn_regr), num_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qGAalfJB8zz"
      },
      "source": [
        "#### Get new image size and augment the image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKhSFbmB2RTo"
      },
      "source": [
        "def get_new_img_size(width, height, img_min_side=300):\n",
        "\tif width <= height:\n",
        "\t\tf = float(img_min_side) / width\n",
        "\t\tresized_height = int(f * height)\n",
        "\t\tresized_width = img_min_side\n",
        "\telse:\n",
        "\t\tf = float(img_min_side) / height\n",
        "\t\tresized_width = int(f * width)\n",
        "\t\tresized_height = img_min_side\n",
        "\n",
        "\treturn resized_width, resized_height\n",
        "\n",
        "def augment(img_data, config, augment=True):\n",
        "\tassert 'filepath' in img_data\n",
        "\tassert 'bboxes' in img_data\n",
        "\tassert 'width' in img_data\n",
        "\tassert 'height' in img_data\n",
        "\n",
        "\timg_data_aug = copy.deepcopy(img_data)\n",
        "\n",
        "\timg = cv2.imread(img_data_aug['filepath'])\n",
        "\n",
        "\tif augment:\n",
        "\t\trows, cols = img.shape[:2]\n",
        "\n",
        "\t\tif config.use_horizontal_flips and np.random.randint(0, 2) == 0:\n",
        "\t\t\timg = cv2.flip(img, 1)\n",
        "\t\t\tfor bbox in img_data_aug['bboxes']:\n",
        "\t\t\t\tx1 = bbox['x1']\n",
        "\t\t\t\tx2 = bbox['x2']\n",
        "\t\t\t\tbbox['x2'] = cols - x1\n",
        "\t\t\t\tbbox['x1'] = cols - x2\n",
        "\n",
        "\t\tif config.use_vertical_flips and np.random.randint(0, 2) == 0:\n",
        "\t\t\timg = cv2.flip(img, 0)\n",
        "\t\t\tfor bbox in img_data_aug['bboxes']:\n",
        "\t\t\t\ty1 = bbox['y1']\n",
        "\t\t\t\ty2 = bbox['y2']\n",
        "\t\t\t\tbbox['y2'] = rows - y1\n",
        "\t\t\t\tbbox['y1'] = rows - y2\n",
        "\n",
        "\t\tif config.rot_90:\n",
        "\t\t\tangle = np.random.choice([0,90,180,270],1)[0]\n",
        "\t\t\tif angle == 270:\n",
        "\t\t\t\timg = np.transpose(img, (1,0,2))\n",
        "\t\t\t\timg = cv2.flip(img, 0)\n",
        "\t\t\telif angle == 180:\n",
        "\t\t\t\timg = cv2.flip(img, -1)\n",
        "\t\t\telif angle == 90:\n",
        "\t\t\t\timg = np.transpose(img, (1,0,2))\n",
        "\t\t\t\timg = cv2.flip(img, 1)\n",
        "\t\t\telif angle == 0:\n",
        "\t\t\t\tpass\n",
        "\n",
        "\t\t\tfor bbox in img_data_aug['bboxes']:\n",
        "\t\t\t\tx1 = bbox['x1']\n",
        "\t\t\t\tx2 = bbox['x2']\n",
        "\t\t\t\ty1 = bbox['y1']\n",
        "\t\t\t\ty2 = bbox['y2']\n",
        "\t\t\t\tif angle == 270:\n",
        "\t\t\t\t\tbbox['x1'] = y1\n",
        "\t\t\t\t\tbbox['x2'] = y2\n",
        "\t\t\t\t\tbbox['y1'] = cols - x2\n",
        "\t\t\t\t\tbbox['y2'] = cols - x1\n",
        "\t\t\t\telif angle == 180:\n",
        "\t\t\t\t\tbbox['x2'] = cols - x1\n",
        "\t\t\t\t\tbbox['x1'] = cols - x2\n",
        "\t\t\t\t\tbbox['y2'] = rows - y1\n",
        "\t\t\t\t\tbbox['y1'] = rows - y2\n",
        "\t\t\t\telif angle == 90:\n",
        "\t\t\t\t\tbbox['x1'] = rows - y2\n",
        "\t\t\t\t\tbbox['x2'] = rows - y1\n",
        "\t\t\t\t\tbbox['y1'] = x1\n",
        "\t\t\t\t\tbbox['y2'] = x2        \n",
        "\t\t\t\telif angle == 0:\n",
        "\t\t\t\t\tpass\n",
        "\n",
        "\timg_data_aug['width'] = img.shape[1]\n",
        "\timg_data_aug['height'] = img.shape[0]\n",
        "\treturn img_data_aug, img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0712o8CXkyh1"
      },
      "source": [
        "#### Generate the ground_truth anchors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvsEv3RIk0cF"
      },
      "source": [
        "def get_anchor_gt(all_img_data, C, img_length_calc_function, mode='train'):\n",
        "\t\"\"\" Yield the ground-truth anchors as Y (labels)\n",
        "\t\t\n",
        "\tArgs:\n",
        "\t\tall_img_data: list(filepath, width, height, list(bboxes))\n",
        "\t\tC: config\n",
        "\t\timg_length_calc_function: function to calculate final layer's feature map (of base model) size according to input image size\n",
        "\t\tmode: 'train' or 'test'; 'train' mode need augmentation\n",
        "\n",
        "\tReturns:\n",
        "\t\tx_img: image data after resized and scaling (smallest size = 300px)\n",
        "\t\tY: [y_rpn_cls, y_rpn_regr]\n",
        "\t\timg_data_aug: augmented image data (original image with augmentation)\n",
        "\t\tdebug_img: show image for debug\n",
        "\t\tnum_pos: show number of positive anchors for debug\n",
        "\t\"\"\"\n",
        "\twhile True:\n",
        "\n",
        "\t\tfor img_data in all_img_data:\n",
        "\t\t\ttry:\n",
        "\n",
        "\t\t\t\t# read in image, and optionally add augmentation\n",
        "\n",
        "\t\t\t\tif mode == 'train':\n",
        "\t\t\t\t\timg_data_aug, x_img = augment(img_data, C, augment=True)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\timg_data_aug, x_img = augment(img_data, C, augment=False)\n",
        "\n",
        "\t\t\t\t(width, height) = (img_data_aug['width'], img_data_aug['height'])\n",
        "\t\t\t\t(rows, cols, _) = x_img.shape\n",
        "\n",
        "\t\t\t\tassert cols == width\n",
        "\t\t\t\tassert rows == height\n",
        "\n",
        "\t\t\t\t# get image dimensions for resizing\n",
        "\t\t\t\t(resized_width, resized_height) = get_new_img_size(width, height, C.im_size)\n",
        "\n",
        "\t\t\t\t# resize the image so that smalles side is length = 300px\n",
        "\t\t\t\tx_img = cv2.resize(x_img, (resized_width, resized_height), interpolation=cv2.INTER_CUBIC)\n",
        "\t\t\t\tdebug_img = x_img.copy()\n",
        "\n",
        "\t\t\t\ttry:\n",
        "\t\t\t\t\ty_rpn_cls, y_rpn_regr, num_pos = calc_rpn(C, img_data_aug, width, height, resized_width, resized_height, img_length_calc_function)\n",
        "\t\t\t\texcept:\n",
        "\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t# Zero-center by mean pixel, and preprocess image\n",
        "\n",
        "\t\t\t\tx_img = x_img[:,:, (2, 1, 0)]  # BGR -> RGB\n",
        "\t\t\t\tx_img = x_img.astype(np.float32)\n",
        "\t\t\t\tx_img[:, :, 0] -= C.img_channel_mean[0]\n",
        "\t\t\t\tx_img[:, :, 1] -= C.img_channel_mean[1]\n",
        "\t\t\t\tx_img[:, :, 2] -= C.img_channel_mean[2]\n",
        "\t\t\t\tx_img /= C.img_scaling_factor\n",
        "\n",
        "\t\t\t\tx_img = np.transpose(x_img, (2, 0, 1))\n",
        "\t\t\t\tx_img = np.expand_dims(x_img, axis=0)\n",
        "\n",
        "\t\t\t\ty_rpn_regr[:, y_rpn_regr.shape[1]//2:, :, :] *= C.std_scaling\n",
        "\n",
        "\t\t\t\tx_img = np.transpose(x_img, (0, 2, 3, 1))\n",
        "\t\t\t\ty_rpn_cls = np.transpose(y_rpn_cls, (0, 2, 3, 1))\n",
        "\t\t\t\ty_rpn_regr = np.transpose(y_rpn_regr, (0, 2, 3, 1))\n",
        "\n",
        "\t\t\t\tyield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug, debug_img, num_pos\n",
        "\n",
        "\t\t\texcept Exception as e:\n",
        "\t\t\t\tprint(e)\n",
        "\t\t\t\tcontinue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZAAMEH4uqu9"
      },
      "source": [
        "#### Define loss functions for all four outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyLxnL4_uvmr"
      },
      "source": [
        "lambda_rpn_regr = 1.0\n",
        "lambda_rpn_class = 1.0\n",
        "\n",
        "lambda_cls_regr = 1.0\n",
        "lambda_cls_class = 1.0\n",
        "\n",
        "epsilon = 1e-4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvGfH6m3yu0_"
      },
      "source": [
        "def rpn_loss_regr(num_anchors):\n",
        "    \"\"\"Loss function for rpn regression\n",
        "    Args:\n",
        "        num_anchors: number of anchors (9 in here)\n",
        "    Returns:\n",
        "        Smooth L1 loss function \n",
        "                           0.5*x*x (if x_abs < 1)\n",
        "                           x_abx - 0.5 (otherwise)\n",
        "    \"\"\"\n",
        "    def rpn_loss_regr_fixed_num(y_true, y_pred):\n",
        "\n",
        "        # x is the difference between true value and predicted vaue\n",
        "        x = y_true[:, :, :, 4 * num_anchors:] - y_pred\n",
        "\n",
        "        # absolute value of x\n",
        "        x_abs = K.abs(x)\n",
        "\n",
        "        # If x_abs <= 1.0, x_bool = 1\n",
        "        x_bool = K.cast(K.less_equal(x_abs, 1.0), tf.float32)\n",
        "\n",
        "        return lambda_rpn_regr * K.sum(\n",
        "            y_true[:, :, :, :4 * num_anchors] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(epsilon + y_true[:, :, :, :4 * num_anchors])\n",
        "\n",
        "    return rpn_loss_regr_fixed_num\n",
        "\n",
        "\n",
        "def rpn_loss_cls(num_anchors):\n",
        "    \"\"\"Loss function for rpn classification\n",
        "    Args:\n",
        "        num_anchors: number of anchors (9 in here)\n",
        "        y_true[:, :, :, :9]: [0,1,0,0,0,0,0,1,0] means only the second and the eighth box is valid which contains pos or neg anchor => isValid\n",
        "        y_true[:, :, :, 9:]: [0,1,0,0,0,0,0,0,0] means the second box is pos and eighth box is negative\n",
        "    Returns:\n",
        "        lambda * sum((binary_crossentropy(isValid*y_pred,y_true))) / N\n",
        "    \"\"\"\n",
        "    def rpn_loss_cls_fixed_num(y_true, y_pred):\n",
        "\n",
        "            return lambda_rpn_class * K.sum(y_true[:, :, :, :num_anchors] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, :, :, num_anchors:])) / K.sum(epsilon + y_true[:, :, :, :num_anchors])\n",
        "\n",
        "    return rpn_loss_cls_fixed_num\n",
        "\n",
        "\n",
        "def class_loss_regr(num_classes):\n",
        "    \"\"\"Loss function for rpn regression\n",
        "    Args:\n",
        "        num_anchors: number of anchors (9 in here)\n",
        "    Returns:\n",
        "        Smooth L1 loss function \n",
        "                           0.5*x*x (if x_abs < 1)\n",
        "                           x_abx - 0.5 (otherwise)\n",
        "    \"\"\"\n",
        "    def class_loss_regr_fixed_num(y_true, y_pred):\n",
        "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
        "        x_abs = K.abs(x)\n",
        "        x_bool = K.cast(K.less_equal(x_abs, 1.0), 'float32')\n",
        "        return lambda_cls_regr * K.sum(y_true[:, :, :4*num_classes] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(epsilon + y_true[:, :, :4*num_classes])\n",
        "    return class_loss_regr_fixed_num\n",
        "\n",
        "\n",
        "def class_loss_cls(y_true, y_pred):\n",
        "    return lambda_cls_class * K.mean(categorical_crossentropy(y_true[0, :, :], y_pred[0, :, :]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cX0N4VDl4zS"
      },
      "source": [
        "def non_max_suppression_fast(boxes, probs, overlap_thresh=0.9, max_boxes=300):\n",
        "    # code used from here: http://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/\n",
        "    # if there are no boxes, return an empty list\n",
        "\n",
        "    # Process explanation:\n",
        "    #   Step 1: Sort the probs list\n",
        "    #   Step 2: Find the larget prob 'Last' in the list and save it to the pick list\n",
        "    #   Step 3: Calculate the IoU with 'Last' box and other boxes in the list. If the IoU is larger than overlap_threshold, delete the box from list\n",
        "    #   Step 4: Repeat step 2 and step 3 until there is no item in the probs list \n",
        "    if len(boxes) == 0:\n",
        "        return []\n",
        "\n",
        "    # grab the coordinates of the bounding boxes\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "\n",
        "    np.testing.assert_array_less(x1, x2)\n",
        "    np.testing.assert_array_less(y1, y2)\n",
        "\n",
        "    # if the bounding boxes integers, convert them to floats --\n",
        "    # this is important since we'll be doing a bunch of divisions\n",
        "    if boxes.dtype.kind == \"i\":\n",
        "        boxes = boxes.astype(\"float\")\n",
        "\n",
        "    # initialize the list of picked indexes\t\n",
        "    pick = []\n",
        "\n",
        "    # calculate the areas\n",
        "    area = (x2 - x1) * (y2 - y1)\n",
        "\n",
        "    # sort the bounding boxes \n",
        "    idxs = np.argsort(probs)\n",
        "\n",
        "    # keep looping while some indexes still remain in the indexes\n",
        "    # list\n",
        "    while len(idxs) > 0:\n",
        "        # grab the last index in the indexes list and add the\n",
        "        # index value to the list of picked indexes\n",
        "        last = len(idxs) - 1\n",
        "        i = idxs[last]\n",
        "        pick.append(i)\n",
        "\n",
        "        # find the intersection\n",
        "\n",
        "        xx1_int = np.maximum(x1[i], x1[idxs[:last]])\n",
        "        yy1_int = np.maximum(y1[i], y1[idxs[:last]])\n",
        "        xx2_int = np.minimum(x2[i], x2[idxs[:last]])\n",
        "        yy2_int = np.minimum(y2[i], y2[idxs[:last]])\n",
        "\n",
        "        ww_int = np.maximum(0, xx2_int - xx1_int)\n",
        "        hh_int = np.maximum(0, yy2_int - yy1_int)\n",
        "\n",
        "        area_int = ww_int * hh_int\n",
        "\n",
        "        # find the union\n",
        "        area_union = area[i] + area[idxs[:last]] - area_int\n",
        "\n",
        "        # compute the ratio of overlap\n",
        "        overlap = area_int/(area_union + 1e-6)\n",
        "\n",
        "        # delete all indexes from the index list that have\n",
        "        idxs = np.delete(idxs, np.concatenate(([last],\n",
        "            np.where(overlap > overlap_thresh)[0])))\n",
        "\n",
        "        if len(pick) >= max_boxes:\n",
        "            break\n",
        "\n",
        "    # return only the bounding boxes that were picked using the integer data type\n",
        "    boxes = boxes[pick].astype(\"int\")\n",
        "    probs = probs[pick]\n",
        "    return boxes, probs\n",
        "\n",
        "def apply_regr_np(X, T):\n",
        "    \"\"\"Apply regression layer to all anchors in one feature map\n",
        "\n",
        "    Args:\n",
        "        X: shape=(4, 18, 25) the current anchor type for all points in the feature map\n",
        "        T: regression layer shape=(4, 18, 25)\n",
        "\n",
        "    Returns:\n",
        "        X: regressed position and size for current anchor\n",
        "    \"\"\"\n",
        "    try:\n",
        "        x = X[0, :, :]\n",
        "        y = X[1, :, :]\n",
        "        w = X[2, :, :]\n",
        "        h = X[3, :, :]\n",
        "\n",
        "        tx = T[0, :, :]\n",
        "        ty = T[1, :, :]\n",
        "        tw = T[2, :, :]\n",
        "        th = T[3, :, :]\n",
        "\n",
        "        cx = x + w/2.\n",
        "        cy = y + h/2.\n",
        "        cx1 = tx * w + cx\n",
        "        cy1 = ty * h + cy\n",
        "\n",
        "        w1 = np.exp(tw.astype(np.float64)) * w\n",
        "        h1 = np.exp(th.astype(np.float64)) * h\n",
        "        x1 = cx1 - w1/2.\n",
        "        y1 = cy1 - h1/2.\n",
        "\n",
        "        x1 = np.round(x1)\n",
        "        y1 = np.round(y1)\n",
        "        w1 = np.round(w1)\n",
        "        h1 = np.round(h1)\n",
        "        return np.stack([x1, y1, w1, h1])\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return X\n",
        "    \n",
        "def apply_regr(x, y, w, h, tx, ty, tw, th):\n",
        "    # Apply regression to x, y, w and h\n",
        "    try:\n",
        "        cx = x + w/2.\n",
        "        cy = y + h/2.\n",
        "        cx1 = tx * w + cx\n",
        "        cy1 = ty * h + cy\n",
        "        w1 = math.exp(tw) * w\n",
        "        h1 = math.exp(th) * h\n",
        "        x1 = cx1 - w1/2.\n",
        "        y1 = cy1 - h1/2.\n",
        "        x1 = int(round(x1))\n",
        "        y1 = int(round(y1))\n",
        "        w1 = int(round(w1))\n",
        "        h1 = int(round(h1))\n",
        "\n",
        "        return x1, y1, w1, h1\n",
        "\n",
        "    except ValueError:\n",
        "        return x, y, w, h\n",
        "    except OverflowError:\n",
        "        return x, y, w, h\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return x, y, w, h\n",
        "\n",
        "def calc_iou(R, img_data, C, class_mapping):\n",
        "    \"\"\"Converts from (x1,y1,x2,y2) to (x,y,w,h) format\n",
        "\n",
        "    Args:\n",
        "        R: bboxes, probs\n",
        "    \"\"\"\n",
        "    bboxes = img_data['bboxes']\n",
        "    (width, height) = (img_data['width'], img_data['height'])\n",
        "    # get image dimensions for resizing\n",
        "    (resized_width, resized_height) = get_new_img_size(width, height, C.im_size)\n",
        "\n",
        "    gta = np.zeros((len(bboxes), 4))\n",
        "\n",
        "    for bbox_num, bbox in enumerate(bboxes):\n",
        "        # get the GT box coordinates, and resize to account for image resizing\n",
        "        # gta[bbox_num, 0] = (40 * (600 / 800)) / 16 = int(round(1.875)) = 2 (x in feature map)\n",
        "        gta[bbox_num, 0] = int(round(bbox['x1'] * (resized_width / float(width))/C.rpn_stride))\n",
        "        gta[bbox_num, 1] = int(round(bbox['x2'] * (resized_width / float(width))/C.rpn_stride))\n",
        "        gta[bbox_num, 2] = int(round(bbox['y1'] * (resized_height / float(height))/C.rpn_stride))\n",
        "        gta[bbox_num, 3] = int(round(bbox['y2'] * (resized_height / float(height))/C.rpn_stride))\n",
        "\n",
        "    x_roi = []\n",
        "    y_class_num = []\n",
        "    y_class_regr_coords = []\n",
        "    y_class_regr_label = []\n",
        "    IoUs = [] # for debugging only\n",
        "\n",
        "    # R.shape[0]: number of bboxes (=300 from non_max_suppression)\n",
        "    for ix in range(R.shape[0]):\n",
        "        (x1, y1, x2, y2) = R[ix, :]\n",
        "        x1 = int(round(x1))\n",
        "        y1 = int(round(y1))\n",
        "        x2 = int(round(x2))\n",
        "        y2 = int(round(y2))\n",
        "\n",
        "        best_iou = 0.0\n",
        "        best_bbox = -1\n",
        "        # Iterate through all the ground-truth bboxes to calculate the iou\n",
        "        for bbox_num in range(len(bboxes)):\n",
        "            curr_iou = iou([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]], [x1, y1, x2, y2])\n",
        "\n",
        "            # Find out the corresponding ground-truth bbox_num with larget iou\n",
        "            if curr_iou > best_iou:\n",
        "                best_iou = curr_iou\n",
        "                best_bbox = bbox_num\n",
        "\n",
        "        if best_iou < C.classifier_min_overlap:\n",
        "                continue\n",
        "        else:\n",
        "            w = x2 - x1\n",
        "            h = y2 - y1\n",
        "            x_roi.append([x1, y1, w, h])\n",
        "            IoUs.append(best_iou)\n",
        "\n",
        "            if C.classifier_min_overlap <= best_iou < C.classifier_max_overlap:\n",
        "                # hard negative example\n",
        "                cls_name = 'bg'\n",
        "            elif C.classifier_max_overlap <= best_iou:\n",
        "                cls_name = bboxes[best_bbox]['class']\n",
        "                cxg = (gta[best_bbox, 0] + gta[best_bbox, 1]) / 2.0\n",
        "                cyg = (gta[best_bbox, 2] + gta[best_bbox, 3]) / 2.0\n",
        "\n",
        "                cx = x1 + w / 2.0\n",
        "                cy = y1 + h / 2.0\n",
        "\n",
        "                tx = (cxg - cx) / float(w)\n",
        "                ty = (cyg - cy) / float(h)\n",
        "                tw = np.log((gta[best_bbox, 1] - gta[best_bbox, 0]) / float(w))\n",
        "                th = np.log((gta[best_bbox, 3] - gta[best_bbox, 2]) / float(h))\n",
        "            else:\n",
        "                print('roi = {}'.format(best_iou))\n",
        "                raise RuntimeError\n",
        "\n",
        "        class_num = class_mapping[cls_name]\n",
        "        class_label = len(class_mapping) * [0]\n",
        "        class_label[class_num] = 1\n",
        "        y_class_num.append(copy.deepcopy(class_label))\n",
        "        coords = [0] * 4 * (len(class_mapping) - 1)\n",
        "        labels = [0] * 4 * (len(class_mapping) - 1)\n",
        "        if cls_name != 'bg':\n",
        "            label_pos = 4 * class_num\n",
        "            sx, sy, sw, sh = C.classifier_regr_std\n",
        "            coords[label_pos:4+label_pos] = [sx*tx, sy*ty, sw*tw, sh*th]\n",
        "            labels[label_pos:4+label_pos] = [1, 1, 1, 1]\n",
        "            y_class_regr_coords.append(copy.deepcopy(coords))\n",
        "            y_class_regr_label.append(copy.deepcopy(labels))\n",
        "        else:\n",
        "            y_class_regr_coords.append(copy.deepcopy(coords))\n",
        "            y_class_regr_label.append(copy.deepcopy(labels))\n",
        "\n",
        "    if len(x_roi) == 0:\n",
        "        return None, None, None, None\n",
        "\n",
        "    # bboxes that iou > C.classifier_min_overlap for all gt bboxes in 300 non_max_suppression bboxes\n",
        "    X = np.array(x_roi)\n",
        "    # one hot code for bboxes from above => x_roi (X)\n",
        "    Y1 = np.array(y_class_num)\n",
        "    # corresponding labels and corresponding gt bboxes\n",
        "    Y2 = np.concatenate([np.array(y_class_regr_label),np.array(y_class_regr_coords)],axis=1)\n",
        "\n",
        "    return np.expand_dims(X, axis=0), np.expand_dims(Y1, axis=0), np.expand_dims(Y2, axis=0), IoUs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vT6X-fqJ1RSl"
      },
      "source": [
        "def rpn_to_roi(rpn_layer, regr_layer, C, dim_ordering, use_regr=True, max_boxes=300,overlap_thresh=0.9):\n",
        "\t\"\"\"Convert rpn layer to roi bboxes\n",
        "\n",
        "\tArgs: (num_anchors = 9)\n",
        "\t\trpn_layer: output layer for rpn classification \n",
        "\t\t\tshape (1, feature_map.height, feature_map.width, num_anchors)\n",
        "\t\t\tMight be (1, 18, 25, 18) if resized image is 400 width and 300\n",
        "\t\tregr_layer: output layer for rpn regression\n",
        "\t\t\tshape (1, feature_map.height, feature_map.width, num_anchors)\n",
        "\t\t\tMight be (1, 18, 25, 72) if resized image is 400 width and 300\n",
        "\t\tC: config\n",
        "\t\tuse_regr: Wether to use bboxes regression in rpn\n",
        "\t\tmax_boxes: max bboxes number for non-max-suppression (NMS)\n",
        "\t\toverlap_thresh: If iou in NMS is larger than this threshold, drop the box\n",
        "\n",
        "\tReturns:\n",
        "\t\tresult: boxes from non-max-suppression (shape=(300, 4))\n",
        "\t\t\tboxes: coordinates for bboxes (on the feature map)\n",
        "\t\"\"\"\n",
        "\tregr_layer = regr_layer / C.std_scaling\n",
        "\n",
        "\tanchor_sizes = C.anchor_box_scales   # (3 in here)\n",
        "\tanchor_ratios = C.anchor_box_ratios  # (3 in here)\n",
        "\n",
        "\tassert rpn_layer.shape[0] == 1\n",
        "\n",
        "\t(rows, cols) = rpn_layer.shape[1:3]\n",
        "\n",
        "\tcurr_layer = 0\n",
        "\n",
        "\t# A.shape = (4, feature_map.height, feature_map.width, num_anchors) \n",
        "\t# Might be (4, 18, 25, 18) if resized image is 400 width and 300\n",
        "\t# A is the coordinates for 9 anchors for every point in the feature map \n",
        "\t# => all 18x25x9=4050 anchors cooridnates\n",
        "\tA = np.zeros((4, rpn_layer.shape[1], rpn_layer.shape[2], rpn_layer.shape[3]))\n",
        "\n",
        "\tfor anchor_size in anchor_sizes:\n",
        "\t\tfor anchor_ratio in anchor_ratios:\n",
        "\t\t\t# anchor_x = (128 * 1) / 16 = 8  => width of current anchor\n",
        "\t\t\t# anchor_y = (128 * 2) / 16 = 16 => height of current anchor\n",
        "\t\t\tanchor_x = (anchor_size * anchor_ratio[0])/C.rpn_stride\n",
        "\t\t\tanchor_y = (anchor_size * anchor_ratio[1])/C.rpn_stride\n",
        "\t\t\t\n",
        "\t\t\t# curr_layer: 0~8 (9 anchors)\n",
        "\t\t\t# the Kth anchor of all position in the feature map (9th in total)\n",
        "\t\t\tregr = regr_layer[0, :, :, 4 * curr_layer:4 * curr_layer + 4] # shape => (18, 25, 4)\n",
        "\t\t\tregr = np.transpose(regr, (2, 0, 1)) # shape => (4, 18, 25)\n",
        "\n",
        "\t\t\t# Create 18x25 mesh grid\n",
        "\t\t\t# For every point in x, there are all the y points and vice versa\n",
        "\t\t\t# X.shape = (18, 25)\n",
        "\t\t\t# Y.shape = (18, 25)\n",
        "\t\t\tX, Y = np.meshgrid(np.arange(cols),np. arange(rows))\n",
        "\n",
        "\t\t\t# Calculate anchor position and size for each feature map point\n",
        "\t\t\tA[0, :, :, curr_layer] = X - anchor_x/2 # Top left x coordinate\n",
        "\t\t\tA[1, :, :, curr_layer] = Y - anchor_y/2 # Top left y coordinate\n",
        "\t\t\tA[2, :, :, curr_layer] = anchor_x       # width of current anchor\n",
        "\t\t\tA[3, :, :, curr_layer] = anchor_y       # height of current anchor\n",
        "\n",
        "\t\t\t# Apply regression to x, y, w and h if there is rpn regression layer\n",
        "\t\t\tif use_regr:\n",
        "\t\t\t\tA[:, :, :, curr_layer] = apply_regr_np(A[:, :, :, curr_layer], regr)\n",
        "\n",
        "\t\t\t# Avoid width and height exceeding 1\n",
        "\t\t\tA[2, :, :, curr_layer] = np.maximum(1, A[2, :, :, curr_layer])\n",
        "\t\t\tA[3, :, :, curr_layer] = np.maximum(1, A[3, :, :, curr_layer])\n",
        "\n",
        "\t\t\t# Convert (x, y , w, h) to (x1, y1, x2, y2)\n",
        "\t\t\t# x1, y1 is top left coordinate\n",
        "\t\t\t# x2, y2 is bottom right coordinate\n",
        "\t\t\tA[2, :, :, curr_layer] += A[0, :, :, curr_layer]\n",
        "\t\t\tA[3, :, :, curr_layer] += A[1, :, :, curr_layer]\n",
        "\n",
        "\t\t\t# Avoid bboxes drawn outside the feature map\n",
        "\t\t\tA[0, :, :, curr_layer] = np.maximum(0, A[0, :, :, curr_layer])\n",
        "\t\t\tA[1, :, :, curr_layer] = np.maximum(0, A[1, :, :, curr_layer])\n",
        "\t\t\tA[2, :, :, curr_layer] = np.minimum(cols-1, A[2, :, :, curr_layer])\n",
        "\t\t\tA[3, :, :, curr_layer] = np.minimum(rows-1, A[3, :, :, curr_layer])\n",
        "\n",
        "\t\t\tcurr_layer += 1\n",
        "\n",
        "\tall_boxes = np.reshape(A.transpose((0, 3, 1, 2)), (4, -1)).transpose((1, 0))  # shape=(4050, 4)\n",
        "\tall_probs = rpn_layer.transpose((0, 3, 1, 2)).reshape((-1))                   # shape=(4050,)\n",
        "\n",
        "\tx1 = all_boxes[:, 0]\n",
        "\ty1 = all_boxes[:, 1]\n",
        "\tx2 = all_boxes[:, 2]\n",
        "\ty2 = all_boxes[:, 3]\n",
        "\n",
        "\t# Find out the bboxes which is illegal and delete them from bboxes list\n",
        "\tidxs = np.where((x1 - x2 >= 0) | (y1 - y2 >= 0))\n",
        "\n",
        "\tall_boxes = np.delete(all_boxes, idxs, 0)\n",
        "\tall_probs = np.delete(all_probs, idxs, 0)\n",
        "\n",
        "\t# Apply non_max_suppression\n",
        "\t# Only extract the bboxes. Don't need rpn probs in the later process\n",
        "\tresult = non_max_suppression_fast(all_boxes, all_probs, overlap_thresh=overlap_thresh, max_boxes=max_boxes)[0]\n",
        "\n",
        "\treturn result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVmMqXE5x70U"
      },
      "source": [
        "### Start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C66bqGuOq7w6"
      },
      "source": [
        "base_path = 'drive/My Drive/AI/Faster_RCNN'\n",
        "\n",
        "if step==2:\n",
        "  train_path =  'drive/My Drive/AI/Faster_RCNN/data2/train_borderless.txt'#step 2\n",
        "elif step==1:\n",
        "  train_path =  'drive/My Drive/AI/Faster_RCNN/table_annotation_with_augmented.txt'#step 1\n",
        "\n",
        "num_rois = 4 # Number of RoIs to process at once.\n",
        "\n",
        "# Augmentation flag\n",
        "horizontal_flips = True # Augment with horizontal flips in training. \n",
        "vertical_flips = True   # Augment with vertical flips in training. \n",
        "rot_90 = True           # Augment with 90 degree rotations in training. \n",
        "\n",
        "name=f'{side_length}x3'\n",
        "output_weight_path = os.path.join(base_path, f'model/model_frcnn_vgg_{name}.hdf5')\n",
        "\n",
        "record_path = os.path.join(base_path, f'model/record_{name}.csv') # Record data (used to save the losses, classification accuracy and mean average precision)\n",
        "\n",
        "base_weight_path = os.path.join(base_path, f'model/vgg16_weights_tf_dim_ordering_tf_kernels.h5')\n",
        "\n",
        "config_output_filename = os.path.join(base_path, f'model_vgg_config_{name}.pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3oAmbbEutH0"
      },
      "source": [
        "# Create the config\n",
        "C = Config()\n",
        "\n",
        "C.use_horizontal_flips = horizontal_flips\n",
        "C.use_vertical_flips = vertical_flips\n",
        "C.rot_90 = rot_90\n",
        "\n",
        "C.record_path = record_path\n",
        "C.model_path = output_weight_path\n",
        "C.num_rois = num_rois\n",
        "\n",
        "C.base_net_weights = base_weight_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiEaAmb-x-so",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8878b04-d6cf-47f1-bfc8-2ca1f6e5c49c"
      },
      "source": [
        "#--------------------------------------------------------#\n",
        "# This step will spend some time to load the data        #\n",
        "#--------------------------------------------------------#\n",
        "st = time.time()\n",
        "\n",
        "if step==1:#train_path=='drive/My Drive/AI/Faster_RCNN/table_annotation_with_augmented.txt':\n",
        "  filename='/content/drive/My Drive/AI/Faster_RCNN/caches_bordered.dat'\n",
        "else:\n",
        "  filename='/content/drive/My Drive/AI/Faster_RCNN/caches_borderless.dat'\n",
        "\n",
        "try:\n",
        "  f=open(filename,'rb')#replace bordered/borderless\n",
        "  k=pickle.load(f)\n",
        "  f.close()\n",
        "except:\n",
        "  k=get_data(train_path)\n",
        "  f=open(filename,'wb')\n",
        "  pickle.dump(k,f)\n",
        "  f.close()\n",
        "\n",
        "train_imgs, classes_count, class_mapping = k\n",
        "print()\n",
        "print('Spend %0.2f mins to load the data' % ((time.time()-st)/60) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Spend 0.00 mins to load the data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-nuSdC56GsK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c5eb4c0-4c1c-463c-f6e8-40d726336d5c"
      },
      "source": [
        "if 'bg' not in classes_count:\n",
        "\tclasses_count['bg'] = 0\n",
        "\tclass_mapping['bg'] = len(class_mapping)\n",
        "# e.g.\n",
        "#    classes_count: {'Car': 2383, 'Mobile phone': 1108, 'Person': 3745, 'bg': 0}\n",
        "#    class_mapping: {'Person': 0, 'Car': 1, 'Mobile phone': 2, 'bg': 3}\n",
        "C.class_mapping = class_mapping\n",
        "\n",
        "print('Training images per class:')\n",
        "pprint.pprint(classes_count)\n",
        "print('Num classes (including bg) = {}'.format(len(classes_count)))\n",
        "print(class_mapping)\n",
        "\n",
        "# Save the configuration\n",
        "with open(config_output_filename, 'wb') as config_f:\n",
        "\tpickle.dump(C,config_f)\n",
        "\tprint('Config has been written to {}, and can be loaded when testing to ensure correct results'.format(config_output_filename))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training images per class:\n",
            "{'Table': 1254, 'bg': 0}\n",
            "Num classes (including bg) = 2\n",
            "{'Table': 0, 'bg': 1}\n",
            "Config has been written to drive/My Drive/AI/Faster_RCNN/model_vgg_config_3x3.pickle, and can be loaded when testing to ensure correct results\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFlq36Sx4F4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81331a9e-4031-4a63-ddb2-8f08eca3466e"
      },
      "source": [
        "# Shuffle the images with seed\n",
        "random.seed(1)\n",
        "random.shuffle(train_imgs)\n",
        "\n",
        "print('Num train samples (images) {}'.format(len(train_imgs)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num train samples (images) 1014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXIV1uXyBo3v"
      },
      "source": [
        "# Get train data generator which generate X, Y, image_data\n",
        "data_gen_train = get_anchor_gt(train_imgs, C, get_img_output_length, mode='train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_yM5jkKqM1G"
      },
      "source": [
        "#### Explore 'data_gen_train'\n",
        "\n",
        "data_gen_train is an **generator**, so we get the data by calling **next(data_gen_train)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIDnio1UlRHi"
      },
      "source": [
        "X, Y, image_data, debug_img, debug_num_pos = next(data_gen_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZXoJ2e3l2Ey",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ddd38dec-32e5-4df2-e57e-fb146049df15"
      },
      "source": [
        "print('Original image: height=%d width=%d'%(image_data['height'], image_data['width']))\n",
        "print('Resized image:  height=%d width=%d C.im_size=%d'%(X.shape[1], X.shape[2], C.im_size))\n",
        "print('Feature map size: height=%d width=%d C.rpn_stride=%d'%(Y[0].shape[1], Y[0].shape[2], C.rpn_stride))\n",
        "print(X.shape)\n",
        "print(str(len(Y))+\" includes 'y_rpn_cls' and 'y_rpn_regr'\")\n",
        "print('Shape of y_rpn_cls {}'.format(Y[0].shape))\n",
        "print('Shape of y_rpn_regr {}'.format(Y[1].shape))\n",
        "print(image_data)\n",
        "\n",
        "print('Number of positive anchors for this image: %d' % (debug_num_pos))\n",
        "if debug_num_pos==0:\n",
        "    gt_x1, gt_x2 = image_data['bboxes'][0]['x1']*(X.shape[2]/image_data['height']), image_data['bboxes'][0]['x2']*(X.shape[2]/image_data['height'])\n",
        "    gt_y1, gt_y2 = image_data['bboxes'][0]['y1']*(X.shape[1]/image_data['width']), image_data['bboxes'][0]['y2']*(X.shape[1]/image_data['width'])\n",
        "    gt_x1, gt_y1, gt_x2, gt_y2 = int(gt_x1), int(gt_y1), int(gt_x2), int(gt_y2)\n",
        "\n",
        "    img = debug_img.copy()\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    color = (0, 255, 0)\n",
        "    cv2.putText(img, 'gt bbox', (gt_x1, gt_y1-5), cv2.FONT_HERSHEY_DUPLEX, 0.7, color, 1)\n",
        "    cv2.rectangle(img, (gt_x1, gt_y1), (gt_x2, gt_y2), color, 2)\n",
        "    cv2.circle(img, (int((gt_x1+gt_x2)/2), int((gt_y1+gt_y2)/2)), 3, color, -1)\n",
        "\n",
        "    plt.grid()\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "else:\n",
        "    cls = Y[0][0]\n",
        "    pos_cls = np.where(cls==1)\n",
        "    print(pos_cls)\n",
        "    regr = Y[1][0]\n",
        "    pos_regr = np.where(regr==1)\n",
        "    print(pos_regr)\n",
        "    print('y_rpn_cls for possible pos anchor: {}'.format(cls[pos_cls[0][0],pos_cls[1][0],:]))\n",
        "    print('y_rpn_regr for positive anchor: {}'.format(regr[pos_regr[0][0],pos_regr[1][0],:]))\n",
        "\n",
        "    gt_x1, gt_x2 = image_data['bboxes'][0]['x1']*(X.shape[2]/image_data['width']), image_data['bboxes'][0]['x2']*(X.shape[2]/image_data['width'])\n",
        "    gt_y1, gt_y2 = image_data['bboxes'][0]['y1']*(X.shape[1]/image_data['height']), image_data['bboxes'][0]['y2']*(X.shape[1]/image_data['height'])\n",
        "    gt_x1, gt_y1, gt_x2, gt_y2 = int(gt_x1), int(gt_y1), int(gt_x2), int(gt_y2)\n",
        "\n",
        "    img = debug_img.copy()\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    color = (0, 255, 0)\n",
        "    #   cv2.putText(img, 'gt bbox', (gt_x1, gt_y1-5), cv2.FONT_HERSHEY_DUPLEX, 0.7, color, 1)\n",
        "    cv2.rectangle(img, (gt_x1, gt_y1), (gt_x2, gt_y2), color, 2)\n",
        "    cv2.circle(img, (int((gt_x1+gt_x2)/2), int((gt_y1+gt_y2)/2)), 3, color, -1)\n",
        "\n",
        "    # Add text\n",
        "    textLabel = 'gt bbox'\n",
        "    (retval,baseLine) = cv2.getTextSize(textLabel,cv2.FONT_HERSHEY_COMPLEX,0.5,1)\n",
        "    textOrg = (gt_x1, gt_y1+5)\n",
        "    cv2.rectangle(img, (textOrg[0] - 5, textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (0, 0, 0), 2)\n",
        "    cv2.rectangle(img, (textOrg[0] - 5,textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (255, 255, 255), -1)\n",
        "    cv2.putText(img, textLabel, textOrg, cv2.FONT_HERSHEY_DUPLEX, 0.5, (0, 0, 0), 1)\n",
        "\n",
        "    # Draw positive anchors according to the y_rpn_regr\n",
        "    for i in range(debug_num_pos):\n",
        "\n",
        "        color = (100+i*(155/4), 0, 100+i*(155/4))\n",
        "\n",
        "        idx = pos_regr[2][i*4]/4\n",
        "        anchor_size = C.anchor_box_scales[int(idx/3)]\n",
        "        anchor_ratio = C.anchor_box_ratios[2-int((idx+1)%3)]\n",
        "\n",
        "        center = (pos_regr[1][i*4]*C.rpn_stride, pos_regr[0][i*4]*C.rpn_stride)\n",
        "        print('Center position of positive anchor: ', center)\n",
        "        cv2.circle(img, center, 3, color, -1)\n",
        "        anc_w, anc_h = anchor_size*anchor_ratio[0], anchor_size*anchor_ratio[1]\n",
        "        cv2.rectangle(img, (center[0]-int(anc_w/2), center[1]-int(anc_h/2)), (center[0]+int(anc_w/2), center[1]+int(anc_h/2)), color, 2)\n",
        "#         cv2.putText(img, 'pos anchor bbox '+str(i+1), (center[0]-int(anc_w/2), center[1]-int(anc_h/2)-5), cv2.FONT_HERSHEY_DUPLEX, 0.5, color, 1)\n",
        "\n",
        "print('Green bboxes is ground-truth bbox. Others are positive anchors')\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.grid()\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original image: height=2560 width=3300\n",
            "Resized image:  height=300 width=386 C.im_size=300\n",
            "Feature map size: height=18 width=24 C.rpn_stride=16\n",
            "(1, 300, 386, 3)\n",
            "2 includes 'y_rpn_cls' and 'y_rpn_regr'\n",
            "Shape of y_rpn_cls (1, 18, 24, 18)\n",
            "Shape of y_rpn_regr (1, 18, 24, 72)\n",
            "{'filepath': '/content/drive/My Drive/AI/Faster_RCNN/data2/dilated_images/5625_015.png', 'width': 3300, 'height': 2560, 'bboxes': [{'class': 'Table', 'x1': 303, 'x2': 2710, 'y1': 297, 'y2': 2067}]}\n",
            "Number of positive anchors for this image: 6\n",
            "(array([ 8,  8,  8,  8,  8,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 13, 14,\n",
            "       16]), array([ 6, 10, 10, 11, 11, 10, 10, 11, 11, 17,  8, 10, 10, 11, 11,  2,  7,\n",
            "        7]), array([ 3,  6, 15,  6, 15,  6, 15,  6, 15,  5,  1,  6, 15,  6, 15,  1,  2,\n",
            "        2]))\n",
            "(array([ 8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10,\n",
            "       10, 10, 10, 10, 10, 10, 10]), array([10, 10, 10, 10, 11, 11, 11, 11, 10, 10, 10, 10, 11, 11, 11, 11, 10,\n",
            "       10, 10, 10, 11, 11, 11, 11]), array([24, 25, 26, 27, 24, 25, 26, 27, 24, 25, 26, 27, 24, 25, 26, 27, 24,\n",
            "       25, 26, 27, 24, 25, 26, 27]))\n",
            "y_rpn_cls for possible pos anchor: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "y_rpn_regr for positive anchor: [ 0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  1.          1.          1.          1.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.12835701  0.03930664  0.38047446 -0.84169073  0.          0.\n",
            "  0.          0.          0.          0.          0.          0.        ]\n",
            "Center position of positive anchor:  (160, 128)\n",
            "Center position of positive anchor:  (176, 128)\n",
            "Center position of positive anchor:  (160, 144)\n",
            "Center position of positive anchor:  (176, 144)\n",
            "Center position of positive anchor:  (160, 160)\n",
            "Center position of positive anchor:  (176, 160)\n",
            "Green bboxes is ground-truth bbox. Others are positive anchors\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4XSyIoubCMY"
      },
      "source": [
        "#### Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jODipXFDnDJ0"
      },
      "source": [
        "input_shape_img = (None, None, 3)\n",
        "\n",
        "img_input = Input(shape=input_shape_img)\n",
        "roi_input = Input(shape=(None, 4))\n",
        "\n",
        "# define the base network (VGG here, can be Resnet50, Inception, etc)\n",
        "shared_layers = nn_base(img_input, trainable=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udTeQMVhfSzw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b1b1764-397a-475e-f0ec-d87299d521b4"
      },
      "source": [
        "# define the RPN, built on the base layers\n",
        "num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios) # 9\n",
        "rpn = rpn_layer(shared_layers, num_anchors)\n",
        "\n",
        "classifier = classifier_layer(shared_layers, roi_input, C.num_rois, nb_classes=len(classes_count))\n",
        "\n",
        "model_rpn = Model(img_input, rpn[:2])\n",
        "model_classifier = Model([img_input, roi_input], classifier)\n",
        "\n",
        "# this is a model that holds both the RPN and the classifier, used to load/save weights for the models\n",
        "model_all = Model([img_input, roi_input], rpn[:2] + classifier)\n",
        "\n",
        "# Because the google colab can only run the session several hours one time (then you need to connect again), \n",
        "# we need to save the model and load the model to continue training\n",
        "if not os.path.isfile(C.model_path):\n",
        "    #If this is the begin of the training, load the pre-traind base network such as vgg-16\n",
        "    try:\n",
        "        print('This is the first time of your training')\n",
        "        print('loading weights from {}'.format(C.base_net_weights))\n",
        "        model_rpn.load_weights(C.base_net_weights, by_name=True)\n",
        "        model_classifier.load_weights(C.base_net_weights, by_name=True)\n",
        "    except:\n",
        "        print('Could not load pretrained model weights. Weights can be found in the keras application folder \\\n",
        "            https://github.com/fchollet/keras/tree/master/keras/applications')\n",
        "    \n",
        "    # Create the record.csv file to record losses, acc and mAP\n",
        "    record_df = pd.DataFrame(columns=['mean_overlapping_bboxes', 'class_acc', 'loss_rpn_cls', 'loss_rpn_regr', 'loss_class_cls', 'loss_class_regr', 'curr_loss', 'elapsed_time', 'mAP'])\n",
        "else:\n",
        "    # If this is a continued training, load the trained model from before\n",
        "    print('Continue training based on previous trained model')\n",
        "    print('Loading weights from {}'.format(C.model_path))\n",
        "    model_rpn.load_weights(C.model_path, by_name=True)\n",
        "    model_classifier.load_weights(C.model_path, by_name=True)\n",
        "    \n",
        "    # Load the records\n",
        "    record_df = pd.read_csv(record_path)\n",
        "\n",
        "    r_mean_overlapping_bboxes = record_df['mean_overlapping_bboxes']\n",
        "    r_class_acc = record_df['class_acc']\n",
        "    r_loss_rpn_cls = record_df['loss_rpn_cls']\n",
        "    r_loss_rpn_regr = record_df['loss_rpn_regr']\n",
        "    r_loss_class_cls = record_df['loss_class_cls']\n",
        "    r_loss_class_regr = record_df['loss_class_regr']\n",
        "    r_curr_loss = record_df['curr_loss']\n",
        "    r_elapsed_time = record_df['elapsed_time']\n",
        "    r_mAP = record_df['mAP']\n",
        "\n",
        "    print('Already train %dK batches'% (len(record_df)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Continue training based on previous trained model\n",
            "Loading weights from drive/My Drive/AI/Faster_RCNN/model/model_frcnn_vgg_3x3.hdf5\n",
            "Already train 147K batches\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ULrg0V1soIR"
      },
      "source": [
        "optimizer = Adam(lr=1e-5)\n",
        "optimizer_classifier = Adam(lr=1e-5)\n",
        "model_rpn.compile(optimizer=optimizer, loss=[rpn_loss_cls(num_anchors), rpn_loss_regr(num_anchors)])\n",
        "model_classifier.compile(optimizer=optimizer_classifier, loss=[class_loss_cls, class_loss_regr(len(classes_count)-1)], metrics={'dense_class_{}'.format(len(classes_count)): 'accuracy'})\n",
        "model_all.compile(optimizer='sgd', loss='mae')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz2BYzL6sqfu"
      },
      "source": [
        "# Training setting\n",
        "total_epochs = len(record_df)\n",
        "r_epochs = len(record_df)\n",
        "\n",
        "epoch_length = 1000\n",
        "num_epochs = 80\n",
        "iter_num = 0\n",
        "\n",
        "#total_epochs += num_epochs\n",
        "total_epochs=80\n",
        "if step==2:\n",
        "  total_epochs=160\n",
        "\n",
        "losses = np.zeros((epoch_length, 5))\n",
        "rpn_accuracy_rpn_monitor = []\n",
        "rpn_accuracy_for_epoch = []\n",
        "\n",
        "if len(record_df)==0:\n",
        "    best_loss = np.Inf\n",
        "else:\n",
        "    best_loss = np.min(r_curr_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDysEDQA2DUz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce36c4ff-ad2c-4d80-e9a4-fb4ed63d8bef"
      },
      "source": [
        "print(len(record_df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "147\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9AQMZYvD2OO"
      },
      "source": [
        "data_gen_train = get_anchor_gt(train_imgs, C, get_img_output_length, mode='train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRXtd5W30DRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ce41e57-c44b-4142-b4fe-a6db32cb4d76"
      },
      "source": [
        "start_time = time.time()\n",
        "for epoch_num in range(r_epochs,total_epochs):\n",
        "\n",
        "    progbar = generic_utils.Progbar(epoch_length)\n",
        "    print('Epoch {}/{}'.format(r_epochs + 1, total_epochs))\n",
        "    \n",
        "    r_epochs += 1\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "\n",
        "            if len(rpn_accuracy_rpn_monitor) == epoch_length and C.verbose:\n",
        "                mean_overlapping_bboxes = float(sum(rpn_accuracy_rpn_monitor))/len(rpn_accuracy_rpn_monitor)\n",
        "                rpn_accuracy_rpn_monitor = []\n",
        "#                 print('Average number of overlapping bounding boxes from RPN = {} for {} previous iterations'.format(mean_overlapping_bboxes, epoch_length))\n",
        "                if mean_overlapping_bboxes == 0:\n",
        "                    print('RPN is not producing bounding boxes that overlap the ground truth boxes. Check RPN settings or keep training.')\n",
        "\n",
        "            # Generate X (x_img) and label Y ([y_rpn_cls, y_rpn_regr])\n",
        "            try:\n",
        "              X, Y, img_data, debug_img, debug_num_pos = next(data_gen_train)\n",
        "            except:\n",
        "              # Get train data generator which generate X, Y, image_data\n",
        "              data_gen_train = get_anchor_gt(train_imgs, C, get_img_output_length, mode='train')\n",
        "              X, Y, img_data, debug_img, debug_num_pos = next(data_gen_train)\n",
        "\n",
        "            # Train rpn model and get loss value [_, loss_rpn_cls, loss_rpn_regr]\n",
        "            loss_rpn = model_rpn.train_on_batch(X, Y)\n",
        "\n",
        "            # Get predicted rpn from rpn model [rpn_cls, rpn_regr]\n",
        "            P_rpn = model_rpn.predict_on_batch(X)\n",
        "\n",
        "            # R: bboxes (shape=(300,4))\n",
        "            # Convert rpn layer to roi bboxes\n",
        "            R = rpn_to_roi(P_rpn[0], P_rpn[1], C, K.image_data_format(), use_regr=True, overlap_thresh=0.7, max_boxes=300)\n",
        "            \n",
        "            # note: calc_iou converts from (x1,y1,x2,y2) to (x,y,w,h) format\n",
        "            # X2: bboxes that iou > C.classifier_min_overlap for all gt bboxes in 300 non_max_suppression bboxes\n",
        "            # Y1: one hot code for bboxes from above => x_roi (X)\n",
        "            # Y2: corresponding labels and corresponding gt bboxes\n",
        "            X2, Y1, Y2, IouS = calc_iou(R, img_data, C, class_mapping)\n",
        "\n",
        "            # If X2 is None means there are no matching bboxes\n",
        "            if X2 is None:\n",
        "                rpn_accuracy_rpn_monitor.append(0)\n",
        "                rpn_accuracy_for_epoch.append(0)\n",
        "                continue\n",
        "            \n",
        "            # Find out the positive anchors and negative anchors\n",
        "            neg_samples = np.where(Y1[0, :, -1] == 1)\n",
        "            pos_samples = np.where(Y1[0, :, -1] == 0)\n",
        "\n",
        "            if len(neg_samples) > 0:\n",
        "                neg_samples = neg_samples[0]\n",
        "            else:\n",
        "                neg_samples = []\n",
        "\n",
        "            if len(pos_samples) > 0:\n",
        "                pos_samples = pos_samples[0]\n",
        "            else:\n",
        "                pos_samples = []\n",
        "\n",
        "            rpn_accuracy_rpn_monitor.append(len(pos_samples))\n",
        "            rpn_accuracy_for_epoch.append((len(pos_samples)))\n",
        "\n",
        "            if C.num_rois > 1:\n",
        "                # If number of positive anchors is larger than 4//2 = 2, randomly choose 2 pos samples\n",
        "                if len(pos_samples) < C.num_rois//2:\n",
        "                    selected_pos_samples = pos_samples.tolist()\n",
        "                else:\n",
        "                    selected_pos_samples = np.random.choice(pos_samples, C.num_rois//2, replace=False).tolist()\n",
        "                \n",
        "                # Randomly choose (num_rois - num_pos) neg samples\n",
        "                try:\n",
        "                    selected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=False).tolist()\n",
        "                except:\n",
        "                    selected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=True).tolist()\n",
        "                \n",
        "                # Save all the pos and neg samples in sel_samples\n",
        "                sel_samples = selected_pos_samples + selected_neg_samples\n",
        "            else:\n",
        "                # in the extreme case where num_rois = 1, we pick a random pos or neg sample\n",
        "                selected_pos_samples = pos_samples.tolist()\n",
        "                selected_neg_samples = neg_samples.tolist()\n",
        "                if np.random.randint(0, 2):\n",
        "                    sel_samples = random.choice(neg_samples)\n",
        "                else:\n",
        "                    sel_samples = random.choice(pos_samples)\n",
        "\n",
        "            # training_data: [X, X2[:, sel_samples, :]]\n",
        "            # labels: [Y1[:, sel_samples, :], Y2[:, sel_samples, :]]\n",
        "            #  X                     => img_data resized image\n",
        "            #  X2[:, sel_samples, :] => num_rois (4 in here) bboxes which contains selected neg and pos\n",
        "            #  Y1[:, sel_samples, :] => one hot encode for num_rois bboxes which contains selected neg and pos\n",
        "            #  Y2[:, sel_samples, :] => labels and gt bboxes for num_rois bboxes which contains selected neg and pos\n",
        "            loss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]], [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])\n",
        "\n",
        "            losses[iter_num, 0] = loss_rpn[1]\n",
        "            losses[iter_num, 1] = loss_rpn[2]\n",
        "\n",
        "            losses[iter_num, 2] = loss_class[1]\n",
        "            losses[iter_num, 3] = loss_class[2]\n",
        "            losses[iter_num, 4] = loss_class[3]\n",
        "\n",
        "            iter_num += 1\n",
        "\n",
        "            progbar.update(iter_num, [('rpn_cls', np.mean(losses[:iter_num, 0])), ('rpn_regr', np.mean(losses[:iter_num, 1])),\n",
        "                                      ('final_cls', np.mean(losses[:iter_num, 2])), ('final_regr', np.mean(losses[:iter_num, 3]))])\n",
        "\n",
        "            if iter_num == epoch_length:\n",
        "                loss_rpn_cls = np.mean(losses[:, 0])\n",
        "                loss_rpn_regr = np.mean(losses[:, 1])\n",
        "                loss_class_cls = np.mean(losses[:, 2])\n",
        "                loss_class_regr = np.mean(losses[:, 3])\n",
        "                class_acc = np.mean(losses[:, 4])\n",
        "\n",
        "                mean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)\n",
        "                rpn_accuracy_for_epoch = []\n",
        "\n",
        "                if C.verbose:\n",
        "                    print('Mean number of bounding boxes from RPN overlapping ground truth boxes: {}'.format(mean_overlapping_bboxes))\n",
        "                    print('Classifier accuracy for bounding boxes from RPN: {}'.format(class_acc))\n",
        "                    print('Loss RPN classifier: {}'.format(loss_rpn_cls))\n",
        "                    print('Loss RPN regression: {}'.format(loss_rpn_regr))\n",
        "                    print('Loss Detector classifier: {}'.format(loss_class_cls))\n",
        "                    print('Loss Detector regression: {}'.format(loss_class_regr))\n",
        "                    print('Total loss: {}'.format(loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr))\n",
        "                    print('Elapsed time: {}'.format(time.time() - start_time))\n",
        "                    elapsed_time = (time.time()-start_time)/60\n",
        "\n",
        "                curr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr\n",
        "                iter_num = 0\n",
        "                start_time = time.time()\n",
        "\n",
        "                if curr_loss < best_loss:\n",
        "                    if C.verbose:\n",
        "                        print('Total loss decreased from {} to {}, saving weights'.format(best_loss,curr_loss))\n",
        "                    best_loss = curr_loss\n",
        "                    model_all.save_weights(C.model_path)\n",
        "\n",
        "                new_row = {'mean_overlapping_bboxes':round(mean_overlapping_bboxes, 3), \n",
        "                           'class_acc':round(class_acc, 3), \n",
        "                           'loss_rpn_cls':round(loss_rpn_cls, 3), \n",
        "                           'loss_rpn_regr':round(loss_rpn_regr, 3), \n",
        "                           'loss_class_cls':round(loss_class_cls, 3), \n",
        "                           'loss_class_regr':round(loss_class_regr, 3), \n",
        "                           'curr_loss':round(curr_loss, 3), \n",
        "                           'elapsed_time':round(elapsed_time, 3), \n",
        "                           'mAP': 0}\n",
        "\n",
        "                record_df = record_df.append(new_row, ignore_index=True)\n",
        "                record_df.to_csv(record_path, index=0)\n",
        "\n",
        "                break\n",
        "\n",
        "        except Exception as e:\n",
        "            print('Exception: {}'.format(e))\n",
        "            continue\n",
        "\n",
        "print('Training complete, exiting.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 148/160\n",
            "   2/1000 [..............................] - ETA: 45:30 - rpn_cls: 4.3040e-04 - rpn_regr: 0.0316 - final_cls: 0.0341 - final_regr: 0.0363  WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_train_function.<locals>.train_function at 0x7fda62ed6d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_train_function.<locals>.train_function at 0x7fd9fafa7bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "   3/1000 [..............................] - ETA: 42:27 - rpn_cls: 4.6337e-04 - rpn_regr: 0.0308 - final_cls: 0.0708 - final_regr: 0.0401WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_train_function.<locals>.train_function at 0x7fda62ed6d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:8 out of the last 8 calls to <function Model.make_train_function.<locals>.train_function at 0x7fd9fafa7bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "  12/1000 [..............................] - ETA: 23:09 - rpn_cls: 4.4975e-04 - rpn_regr: 0.0270 - final_cls: 0.1486 - final_regr: 0.0430Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 391/1000 [==========>...................] - ETA: 8:31 - rpn_cls: 0.1191 - rpn_regr: 0.0280 - final_cls: 0.1413 - final_regr: 0.0508Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 643/1000 [==================>...........] - ETA: 4:55 - rpn_cls: 0.1237 - rpn_regr: 0.0289 - final_cls: 0.1393 - final_regr: 0.0503Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 819/1000 [=======================>......] - ETA: 2:28 - rpn_cls: 0.1248 - rpn_regr: 0.0294 - final_cls: 0.1386 - final_regr: 0.0502Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            "1000/1000 [==============================] - 827s 818ms/step - rpn_cls: 0.1264 - rpn_regr: 0.0296 - final_cls: 0.1380 - final_regr: 0.0501\n",
            "Mean number of bounding boxes from RPN overlapping ground truth boxes: 13.424453280318092\n",
            "Classifier accuracy for bounding boxes from RPN: 0.9465\n",
            "Loss RPN classifier: 0.12914424206028013\n",
            "Loss RPN regression: 0.03095115524926223\n",
            "Loss Detector classifier: 0.1362415962843588\n",
            "Loss Detector regression: 0.04923485861904919\n",
            "Total loss: 0.3455718522129504\n",
            "Elapsed time: 827.3481869697571\n",
            "Epoch 149/160\n",
            "  20/1000 [..............................] - ETA: 7:56 - rpn_cls: 0.1378 - rpn_regr: 0.0264 - final_cls: 0.1384 - final_regr: 0.0448Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            "  30/1000 [..............................] - ETA: 7:14 - rpn_cls: 0.2249 - rpn_regr: 0.0262 - final_cls: 0.1448 - final_regr: 0.0454Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 372/1000 [==========>...................] - ETA: 3:40 - rpn_cls: 0.1751 - rpn_regr: 0.0314 - final_cls: 0.1441 - final_regr: 0.0555Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 398/1000 [==========>...................] - ETA: 3:31 - rpn_cls: 0.1721 - rpn_regr: 0.0315 - final_cls: 0.1445 - final_regr: 0.0554Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 404/1000 [===========>..................] - ETA: 3:29 - rpn_cls: 0.1716 - rpn_regr: 0.0315 - final_cls: 0.1446 - final_regr: 0.0554Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 456/1000 [============>.................] - ETA: 3:11 - rpn_cls: 0.1689 - rpn_regr: 0.0316 - final_cls: 0.1451 - final_regr: 0.0552Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 534/1000 [===============>..............] - ETA: 2:43 - rpn_cls: 0.1648 - rpn_regr: 0.0317 - final_cls: 0.1453 - final_regr: 0.0548Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 708/1000 [====================>.........] - ETA: 1:42 - rpn_cls: 0.1534 - rpn_regr: 0.0317 - final_cls: 0.1454 - final_regr: 0.0540Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            "1000/1000 [==============================] - 348s 347ms/step - rpn_cls: 0.1429 - rpn_regr: 0.0316 - final_cls: 0.1447 - final_regr: 0.0531\n",
            "Mean number of bounding boxes from RPN overlapping ground truth boxes: 13.384539147670962\n",
            "Classifier accuracy for bounding boxes from RPN: 0.94475\n",
            "Loss RPN classifier: 0.11338366018409243\n",
            "Loss RPN regression: 0.031512519016338045\n",
            "Loss Detector classifier: 0.14716541105433498\n",
            "Loss Detector regression: 0.0513203519275412\n",
            "Total loss: 0.3433819421823066\n",
            "Elapsed time: 347.55504179000854\n",
            "Epoch 150/160\n",
            "  36/1000 [>.............................] - ETA: 5:22 - rpn_cls: 0.0030 - rpn_regr: 0.0391 - final_cls: 0.0878 - final_regr: 0.0428Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            "  91/1000 [=>............................] - ETA: 5:18 - rpn_cls: 0.0168 - rpn_regr: 0.0336 - final_cls: 0.1004 - final_regr: 0.0441Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 411/1000 [===========>..................] - ETA: 3:21 - rpn_cls: 0.0692 - rpn_regr: 0.0283 - final_cls: 0.1167 - final_regr: 0.0451Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 463/1000 [============>.................] - ETA: 3:03 - rpn_cls: 0.0681 - rpn_regr: 0.0281 - final_cls: 0.1168 - final_regr: 0.0452Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 541/1000 [===============>..............] - ETA: 2:37 - rpn_cls: 0.0688 - rpn_regr: 0.0280 - final_cls: 0.1178 - final_regr: 0.0456Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 715/1000 [====================>.........] - ETA: 1:38 - rpn_cls: 0.0751 - rpn_regr: 0.0279 - final_cls: 0.1203 - final_regr: 0.0464Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 761/1000 [=====================>........] - ETA: 1:22 - rpn_cls: 0.0763 - rpn_regr: 0.0279 - final_cls: 0.1209 - final_regr: 0.0465Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            "1000/1000 [==============================] - 342s 342ms/step - rpn_cls: 0.0798 - rpn_regr: 0.0279 - final_cls: 0.1230 - final_regr: 0.0471\n",
            "Mean number of bounding boxes from RPN overlapping ground truth boxes: 13.479642502482621\n",
            "Classifier accuracy for bounding boxes from RPN: 0.94825\n",
            "Loss RPN classifier: 0.08561040091615658\n",
            "Loss RPN regression: 0.028215328778103867\n",
            "Loss Detector classifier: 0.1271173463283849\n",
            "Loss Detector regression: 0.04921613645926118\n",
            "Total loss: 0.29015921248190657\n",
            "Elapsed time: 342.4344997406006\n",
            "Epoch 151/160\n",
            "  43/1000 [>.............................] - ETA: 5:18 - rpn_cls: 0.0736 - rpn_regr: 0.0260 - final_cls: 0.0722 - final_regr: 0.0420Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 417/1000 [===========>..................] - ETA: 3:19 - rpn_cls: 0.0620 - rpn_regr: 0.0255 - final_cls: 0.1000 - final_regr: 0.0457Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 638/1000 [==================>...........] - ETA: 2:04 - rpn_cls: 0.0726 - rpn_regr: 0.0265 - final_cls: 0.1062 - final_regr: 0.0462Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 722/1000 [====================>.........] - ETA: 1:35 - rpn_cls: 0.0757 - rpn_regr: 0.0268 - final_cls: 0.1076 - final_regr: 0.0464Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 837/1000 [========================>.....] - ETA: 56s - rpn_cls: 0.0809 - rpn_regr: 0.0272 - final_cls: 0.1096 - final_regr: 0.0467Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            "1000/1000 [==============================] - 346s 345ms/step - rpn_cls: 0.0872 - rpn_regr: 0.0276 - final_cls: 0.1122 - final_regr: 0.0471\n",
            "Mean number of bounding boxes from RPN overlapping ground truth boxes: 13.600198412698413\n",
            "Classifier accuracy for bounding boxes from RPN: 0.95\n",
            "Loss RPN classifier: 0.11472853196848358\n",
            "Loss RPN regression: 0.030422808873408938\n",
            "Loss Detector classifier: 0.12513700105745557\n",
            "Loss Detector regression: 0.04931315312371589\n",
            "Total loss: 0.31960149502306395\n",
            "Elapsed time: 345.548335313797\n",
            "Epoch 152/160\n",
            "  38/1000 [>.............................] - ETA: 5:19 - rpn_cls: 0.4721 - rpn_regr: 0.0403 - final_cls: 0.1180 - final_regr: 0.0519Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            "  48/1000 [>.............................] - ETA: 5:29 - rpn_cls: 0.4365 - rpn_regr: 0.0380 - final_cls: 0.1172 - final_regr: 0.0525Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 389/1000 [==========>...................] - ETA: 3:32 - rpn_cls: 0.1937 - rpn_regr: 0.0292 - final_cls: 0.1359 - final_regr: 0.0526Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 415/1000 [===========>..................] - ETA: 3:24 - rpn_cls: 0.1903 - rpn_regr: 0.0291 - final_cls: 0.1358 - final_regr: 0.0525Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 667/1000 [===================>..........] - ETA: 1:56 - rpn_cls: 0.1746 - rpn_regr: 0.0286 - final_cls: 0.1338 - final_regr: 0.0518Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 774/1000 [======================>.......] - ETA: 1:19 - rpn_cls: 0.1737 - rpn_regr: 0.0286 - final_cls: 0.1337 - final_regr: 0.0518Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 843/1000 [========================>.....] - ETA: 54s - rpn_cls: 0.1742 - rpn_regr: 0.0287 - final_cls: 0.1338 - final_regr: 0.0519Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 983/1000 [============================>.] - ETA: 5s - rpn_cls: 0.1734 - rpn_regr: 0.0288 - final_cls: 0.1340 - final_regr: 0.0521Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            "1000/1000 [==============================] - 351s 351ms/step - rpn_cls: 0.1731 - rpn_regr: 0.0289 - final_cls: 0.1341 - final_regr: 0.0521\n",
            "Mean number of bounding boxes from RPN overlapping ground truth boxes: 13.387512388503469\n",
            "Classifier accuracy for bounding boxes from RPN: 0.94675\n",
            "Loss RPN classifier: 0.16247832974489115\n",
            "Loss RPN regression: 0.030643746782938252\n",
            "Loss Detector classifier: 0.13880442916349056\n",
            "Loss Detector regression: 0.05331303471140564\n",
            "Total loss: 0.3852395404027256\n",
            "Elapsed time: 350.8520174026489\n",
            "Epoch 153/160\n",
            " 423/1000 [===========>..................] - ETA: 3:22 - rpn_cls: 0.0634 - rpn_regr: 0.0302 - final_cls: 0.1078 - final_regr: 0.0507Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 561/1000 [===============>..............] - ETA: 2:33 - rpn_cls: 0.0674 - rpn_regr: 0.0300 - final_cls: 0.1102 - final_regr: 0.0503Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 735/1000 [=====================>........] - ETA: 1:32 - rpn_cls: 0.0755 - rpn_regr: 0.0297 - final_cls: 0.1129 - final_regr: 0.0500Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            "1000/1000 [==============================] - 349s 349ms/step - rpn_cls: 0.0883 - rpn_regr: 0.0295 - final_cls: 0.1148 - final_regr: 0.0499\n",
            "Mean number of bounding boxes from RPN overlapping ground truth boxes: 13.487051792828685\n",
            "Classifier accuracy for bounding boxes from RPN: 0.9545\n",
            "Loss RPN classifier: 0.13072960286749852\n",
            "Loss RPN regression: 0.028978425388922913\n",
            "Loss Detector classifier: 0.1189776114481674\n",
            "Loss Detector regression: 0.04972736271936446\n",
            "Total loss: 0.32841300242395327\n",
            "Elapsed time: 349.11922574043274\n",
            "Epoch 154/160\n",
            "  53/1000 [>.............................] - ETA: 5:30 - rpn_cls: 0.0656 - rpn_regr: 0.0281 - final_cls: 0.1641 - final_regr: 0.0427Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 493/1000 [=============>................] - ETA: 2:57 - rpn_cls: 0.1095 - rpn_regr: 0.0302 - final_cls: 0.1426 - final_regr: 0.0521Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 661/1000 [==================>...........] - ETA: 1:58 - rpn_cls: 0.1087 - rpn_regr: 0.0306 - final_cls: 0.1425 - final_regr: 0.0526Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 684/1000 [===================>..........] - ETA: 1:50 - rpn_cls: 0.1086 - rpn_regr: 0.0306 - final_cls: 0.1425 - final_regr: 0.0526Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 744/1000 [=====================>........] - ETA: 1:29 - rpn_cls: 0.1085 - rpn_regr: 0.0307 - final_cls: 0.1423 - final_regr: 0.0527Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 790/1000 [======================>.......] - ETA: 1:13 - rpn_cls: 0.1084 - rpn_regr: 0.0307 - final_cls: 0.1423 - final_regr: 0.0528Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 859/1000 [========================>.....] - ETA: 49s - rpn_cls: 0.1081 - rpn_regr: 0.0307 - final_cls: 0.1423 - final_regr: 0.0529Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 981/1000 [============================>.] - ETA: 6s - rpn_cls: 0.1073 - rpn_regr: 0.0307 - final_cls: 0.1427 - final_regr: 0.0528Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            "1000/1000 [==============================] - 351s 351ms/step - rpn_cls: 0.1072 - rpn_regr: 0.0307 - final_cls: 0.1428 - final_regr: 0.0528\n",
            "Mean number of bounding boxes from RPN overlapping ground truth boxes: 13.60218253968254\n",
            "Classifier accuracy for bounding boxes from RPN: 0.94125\n",
            "Loss RPN classifier: 0.10127275345294579\n",
            "Loss RPN regression: 0.030215952666592785\n",
            "Loss Detector classifier: 0.1495924726173871\n",
            "Loss Detector regression: 0.052353273989167066\n",
            "Total loss: 0.3334344527260927\n",
            "Elapsed time: 351.2814300060272\n",
            "Epoch 155/160\n",
            "  59/1000 [>.............................] - ETA: 5:17 - rpn_cls: 0.1405 - rpn_regr: 0.0366 - final_cls: 0.1690 - final_regr: 0.0525Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 410/1000 [===========>..................] - ETA: 3:26 - rpn_cls: 0.0562 - rpn_regr: 0.0291 - final_cls: 0.1375 - final_regr: 0.0476Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 436/1000 [============>.................] - ETA: 3:17 - rpn_cls: 0.0560 - rpn_regr: 0.0290 - final_cls: 0.1366 - final_regr: 0.0476Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 442/1000 [============>.................] - ETA: 3:16 - rpn_cls: 0.0559 - rpn_regr: 0.0290 - final_cls: 0.1365 - final_regr: 0.0476Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 573/1000 [================>.............] - ETA: 2:30 - rpn_cls: 0.0548 - rpn_regr: 0.0285 - final_cls: 0.1333 - final_regr: 0.0473Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 662/1000 [==================>...........] - ETA: 1:58 - rpn_cls: 0.0559 - rpn_regr: 0.0285 - final_cls: 0.1328 - final_regr: 0.0472Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 746/1000 [=====================>........] - ETA: 1:29 - rpn_cls: 0.0581 - rpn_regr: 0.0284 - final_cls: 0.1328 - final_regr: 0.0471Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 985/1000 [============================>.] - ETA: 5s - rpn_cls: 0.0628 - rpn_regr: 0.0283 - final_cls: 0.1325 - final_regr: 0.0473Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            "1000/1000 [==============================] - 351s 351ms/step - rpn_cls: 0.0630 - rpn_regr: 0.0283 - final_cls: 0.1325 - final_regr: 0.0473\n",
            "Mean number of bounding boxes from RPN overlapping ground truth boxes: 13.718811881188119\n",
            "Classifier accuracy for bounding boxes from RPN: 0.9465\n",
            "Loss RPN classifier: 0.07504032841152562\n",
            "Loss RPN regression: 0.02780537814638228\n",
            "Loss Detector classifier: 0.1301902979924721\n",
            "Loss Detector regression: 0.04800553540117107\n",
            "Total loss: 0.28104153995155107\n",
            "Elapsed time: 351.4301688671112\n",
            "Epoch 156/160\n",
            "  63/1000 [>.............................] - ETA: 5:26 - rpn_cls: 0.0044 - rpn_regr: 0.0321 - final_cls: 0.1154 - final_regr: 0.0502Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 234/1000 [======>.......................] - ETA: 4:27 - rpn_cls: 0.1096 - rpn_regr: 0.0287 - final_cls: 0.1236 - final_regr: 0.0469Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 442/1000 [============>.................] - ETA: 3:14 - rpn_cls: 0.1154 - rpn_regr: 0.0280 - final_cls: 0.1249 - final_regr: 0.0451Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 501/1000 [==============>...............] - ETA: 2:53 - rpn_cls: 0.1170 - rpn_regr: 0.0279 - final_cls: 0.1245 - final_regr: 0.0449Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 579/1000 [================>.............] - ETA: 2:27 - rpn_cls: 0.1182 - rpn_regr: 0.0279 - final_cls: 0.1245 - final_regr: 0.0448Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 626/1000 [=================>............] - ETA: 2:11 - rpn_cls: 0.1185 - rpn_regr: 0.0278 - final_cls: 0.1245 - final_regr: 0.0448Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 691/1000 [===================>..........] - ETA: 1:48 - rpn_cls: 0.1184 - rpn_regr: 0.0278 - final_cls: 0.1245 - final_regr: 0.0449Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 751/1000 [=====================>........] - ETA: 1:27 - rpn_cls: 0.1180 - rpn_regr: 0.0278 - final_cls: 0.1248 - final_regr: 0.0450Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 867/1000 [=========================>....] - ETA: 46s - rpn_cls: 0.1160 - rpn_regr: 0.0277 - final_cls: 0.1252 - final_regr: 0.0451Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 981/1000 [============================>.] - ETA: 6s - rpn_cls: 0.1150 - rpn_regr: 0.0278 - final_cls: 0.1253 - final_regr: 0.0453Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            "1000/1000 [==============================] - 352s 352ms/step - rpn_cls: 0.1150 - rpn_regr: 0.0278 - final_cls: 0.1253 - final_regr: 0.0454\n",
            "Mean number of bounding boxes from RPN overlapping ground truth boxes: 13.472277227722772\n",
            "Classifier accuracy for bounding boxes from RPN: 0.95325\n",
            "Loss RPN classifier: 0.11287399055683008\n",
            "Loss RPN regression: 0.028490944390097864\n",
            "Loss Detector classifier: 0.1258619135965928\n",
            "Loss Detector regression: 0.04721709675807506\n",
            "Total loss: 0.3144439453015958\n",
            "Elapsed time: 351.9229588508606\n",
            "Epoch 157/160\n",
            "  78/1000 [=>............................] - ETA: 5:16 - rpn_cls: 0.0793 - rpn_regr: 0.0470 - final_cls: 0.1122 - final_regr: 0.0520Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 133/1000 [==>...........................] - ETA: 5:03 - rpn_cls: 0.0699 - rpn_regr: 0.0433 - final_cls: 0.1069 - final_regr: 0.0533Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 419/1000 [===========>..................] - ETA: 3:23 - rpn_cls: 0.0827 - rpn_regr: 0.0365 - final_cls: 0.1252 - final_regr: 0.0535Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 445/1000 [============>.................] - ETA: 3:14 - rpn_cls: 0.0833 - rpn_regr: 0.0363 - final_cls: 0.1258 - final_regr: 0.0533Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 451/1000 [============>.................] - ETA: 3:12 - rpn_cls: 0.0834 - rpn_regr: 0.0362 - final_cls: 0.1259 - final_regr: 0.0533Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 672/1000 [===================>..........] - ETA: 1:55 - rpn_cls: 0.0832 - rpn_regr: 0.0347 - final_cls: 0.1263 - final_regr: 0.0521Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 756/1000 [=====================>........] - ETA: 1:25 - rpn_cls: 0.0837 - rpn_regr: 0.0343 - final_cls: 0.1268 - final_regr: 0.0518Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 872/1000 [=========================>....] - ETA: 45s - rpn_cls: 0.0880 - rpn_regr: 0.0339 - final_cls: 0.1274 - final_regr: 0.0515Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 896/1000 [=========================>....] - ETA: 36s - rpn_cls: 0.0890 - rpn_regr: 0.0338 - final_cls: 0.1274 - final_regr: 0.0514Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 993/1000 [============================>.] - ETA: 2s - rpn_cls: 0.0928 - rpn_regr: 0.0335 - final_cls: 0.1278 - final_regr: 0.0512Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            "1000/1000 [==============================] - 354s 354ms/step - rpn_cls: 0.0931 - rpn_regr: 0.0335 - final_cls: 0.1279 - final_regr: 0.0512\n",
            "Mean number of bounding boxes from RPN overlapping ground truth boxes: 13.492079207920792\n",
            "Classifier accuracy for bounding boxes from RPN: 0.9495\n",
            "Loss RPN classifier: 0.1329488110276104\n",
            "Loss RPN regression: 0.03086543787093251\n",
            "Loss Detector classifier: 0.13198462937972363\n",
            "Loss Detector regression: 0.04979669833893422\n",
            "Total loss: 0.3455955766172008\n",
            "Elapsed time: 353.6509790420532\n",
            "Epoch 158/160\n",
            " 138/1000 [===>..........................] - ETA: 5:08 - rpn_cls: 0.1137 - rpn_regr: 0.0293 - final_cls: 0.1378 - final_regr: 0.0515Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 424/1000 [===========>..................] - ETA: 3:25 - rpn_cls: 0.1236 - rpn_regr: 0.0293 - final_cls: 0.1381 - final_regr: 0.0531Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 457/1000 [============>.................] - ETA: 3:14 - rpn_cls: 0.1244 - rpn_regr: 0.0294 - final_cls: 0.1376 - final_regr: 0.0530Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 588/1000 [================>.............] - ETA: 2:27 - rpn_cls: 0.1276 - rpn_regr: 0.0296 - final_cls: 0.1377 - final_regr: 0.0530Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 939/1000 [===========================>..] - ETA: 21s - rpn_cls: 0.1264 - rpn_regr: 0.0298 - final_cls: 0.1367 - final_regr: 0.0531Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            "1000/1000 [==============================] - 356s 356ms/step - rpn_cls: 0.1266 - rpn_regr: 0.0298 - final_cls: 0.1368 - final_regr: 0.0532\n",
            "Mean number of bounding boxes from RPN overlapping ground truth boxes: 13.459244532803181\n",
            "Classifier accuracy for bounding boxes from RPN: 0.943\n",
            "Loss RPN classifier: 0.1252571234548949\n",
            "Loss RPN regression: 0.0299081829295028\n",
            "Loss Detector classifier: 0.13819346286781364\n",
            "Loss Detector regression: 0.053228496592026206\n",
            "Total loss: 0.34658726584423755\n",
            "Elapsed time: 355.9723198413849\n",
            "Epoch 159/160\n",
            "Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            "  89/1000 [=>............................] - ETA: 5:07 - rpn_cls: 0.0857 - rpn_regr: 0.0403 - final_cls: 0.0823 - final_regr: 0.0599Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 254/1000 [======>.......................] - ETA: 4:25 - rpn_cls: 0.0988 - rpn_regr: 0.0374 - final_cls: 0.1059 - final_regr: 0.0546Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 255/1000 [======>.......................] - ETA: 4:26 - rpn_cls: 0.0990 - rpn_regr: 0.0374 - final_cls: 0.1059 - final_regr: 0.0546Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 427/1000 [===========>..................] - ETA: 3:24 - rpn_cls: 0.1037 - rpn_regr: 0.0353 - final_cls: 0.1161 - final_regr: 0.0526Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 453/1000 [============>.................] - ETA: 3:15 - rpn_cls: 0.1035 - rpn_regr: 0.0350 - final_cls: 0.1171 - final_regr: 0.0523Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 459/1000 [============>.................] - ETA: 3:14 - rpn_cls: 0.1035 - rpn_regr: 0.0350 - final_cls: 0.1173 - final_regr: 0.0523Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 511/1000 [==============>...............] - ETA: 2:55 - rpn_cls: 0.1046 - rpn_regr: 0.0345 - final_cls: 0.1188 - final_regr: 0.0518Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 589/1000 [================>.............] - ETA: 2:27 - rpn_cls: 0.1066 - rpn_regr: 0.0339 - final_cls: 0.1201 - final_regr: 0.0513Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 678/1000 [===================>..........] - ETA: 1:55 - rpn_cls: 0.1070 - rpn_regr: 0.0333 - final_cls: 0.1213 - final_regr: 0.0508Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 701/1000 [====================>.........] - ETA: 1:47 - rpn_cls: 0.1071 - rpn_regr: 0.0332 - final_cls: 0.1216 - final_regr: 0.0507Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 877/1000 [=========================>....] - ETA: 44s - rpn_cls: 0.1070 - rpn_regr: 0.0323 - final_cls: 0.1233 - final_regr: 0.0502Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            "1000/1000 [==============================] - 359s 358ms/step - rpn_cls: 0.1072 - rpn_regr: 0.0318 - final_cls: 0.1241 - final_regr: 0.0500\n",
            "Mean number of bounding boxes from RPN overlapping ground truth boxes: 13.545812807881774\n",
            "Classifier accuracy for bounding boxes from RPN: 0.94775\n",
            "Loss RPN classifier: 0.11763167305864579\n",
            "Loss RPN regression: 0.02796082437070436\n",
            "Loss Detector classifier: 0.13135034756726963\n",
            "Loss Detector regression: 0.04851170024985913\n",
            "Total loss: 0.32545454524647893\n",
            "Elapsed time: 358.98441553115845\n",
            "Epoch 160/160\n",
            "  89/1000 [=>............................] - ETA: 5:19 - rpn_cls: 0.4181 - rpn_regr: 0.0311 - final_cls: 0.1170 - final_regr: 0.0501Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 430/1000 [===========>..................] - ETA: 3:21 - rpn_cls: 0.1902 - rpn_regr: 0.0291 - final_cls: 0.1337 - final_regr: 0.0468Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 456/1000 [============>.................] - ETA: 3:12 - rpn_cls: 0.1849 - rpn_regr: 0.0290 - final_cls: 0.1335 - final_regr: 0.0468Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 462/1000 [============>.................] - ETA: 3:10 - rpn_cls: 0.1837 - rpn_regr: 0.0290 - final_cls: 0.1335 - final_regr: 0.0468Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 514/1000 [==============>...............] - ETA: 2:52 - rpn_cls: 0.1750 - rpn_regr: 0.0288 - final_cls: 0.1335 - final_regr: 0.0468Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 682/1000 [===================>..........] - ETA: 1:53 - rpn_cls: 0.1551 - rpn_regr: 0.0284 - final_cls: 0.1329 - final_regr: 0.0472Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 705/1000 [====================>.........] - ETA: 1:45 - rpn_cls: 0.1529 - rpn_regr: 0.0283 - final_cls: 0.1328 - final_regr: 0.0472Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 765/1000 [=====================>........] - ETA: 1:24 - rpn_cls: 0.1478 - rpn_regr: 0.0282 - final_cls: 0.1327 - final_regr: 0.0473Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 811/1000 [=======================>......] - ETA: 1:07 - rpn_cls: 0.1445 - rpn_regr: 0.0282 - final_cls: 0.1325 - final_regr: 0.0473Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            " 880/1000 [=========================>....] - ETA: 42s - rpn_cls: 0.1399 - rpn_regr: 0.0281 - final_cls: 0.1324 - final_regr: 0.0473Exception: in user code:\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    <ipython-input-19-84f9950f7cd7>:53 class_loss_regr_fixed_num  *\n",
            "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1180 binary_op_wrapper\n",
            "        raise e\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1164 binary_op_wrapper\n",
            "        return func(x, y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
            "        return target(*args, **kwargs)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:561 subtract\n",
            "        return gen_math_ops.sub(x, y, name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:10317 sub\n",
            "        \"Sub\", x=x, y=y, name=name)\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\n",
            "        inferred_from[input_arg.type_attr]))\n",
            "\n",
            "    TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type int64 of argument 'x'.\n",
            "\n",
            "1000/1000 [==============================] - 358s 358ms/step - rpn_cls: 0.1338 - rpn_regr: 0.0280 - final_cls: 0.1324 - final_regr: 0.0473\n",
            "Mean number of bounding boxes from RPN overlapping ground truth boxes: 13.439169139465875\n",
            "Classifier accuracy for bounding boxes from RPN: 0.94625\n",
            "Loss RPN classifier: 0.0857591079290359\n",
            "Loss RPN regression: 0.027006997845950537\n",
            "Loss Detector classifier: 0.13285985949743553\n",
            "Loss Detector regression: 0.04757907960389275\n",
            "Total loss: 0.2932050448763147\n",
            "Elapsed time: 358.0902805328369\n",
            "Training complete, exiting.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt-1Grs90oD3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "27805c4f-84d9-40e7-ca6f-0c93687db5d3"
      },
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(np.arange(0, r_epochs), record_df['mean_overlapping_bboxes'], 'r')\n",
        "plt.title('mean_overlapping_bboxes')\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(np.arange(0, r_epochs), record_df['class_acc'], 'r')\n",
        "plt.title('class_acc')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(np.arange(0, r_epochs), record_df['loss_rpn_cls'], 'r')\n",
        "plt.title('loss_rpn_cls')\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(np.arange(0, r_epochs), record_df['loss_rpn_regr'], 'r')\n",
        "plt.title('loss_rpn_regr')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(np.arange(0, r_epochs), record_df['loss_class_cls'], 'r')\n",
        "plt.title('loss_class_cls')\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(np.arange(0, r_epochs), record_df['loss_class_regr'], 'r')\n",
        "plt.title('loss_class_regr')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.arange(0, r_epochs), record_df['curr_loss'], 'r')\n",
        "plt.title('total_loss')\n",
        "plt.show()\n",
        "\n",
        "# plt.figure(figsize=(15,5))\n",
        "# plt.subplot(1,2,1)\n",
        "# plt.plot(np.arange(0, r_epochs), record_df['curr_loss'], 'r')\n",
        "# plt.title('total_loss')\n",
        "# plt.subplot(1,2,2)\n",
        "# plt.plot(np.arange(0, r_epochs), record_df['elapsed_time'], 'r')\n",
        "# plt.title('elapsed_time')\n",
        "# plt.show()\n",
        "\n",
        "# plt.title('loss')\n",
        "# plt.plot(np.arange(0, r_epochs), record_df['loss_rpn_cls'], 'b')\n",
        "# plt.plot(np.arange(0, r_epochs), record_df['loss_rpn_regr'], 'g')\n",
        "# plt.plot(np.arange(0, r_epochs), record_df['loss_class_cls'], 'r')\n",
        "# plt.plot(np.arange(0, r_epochs), record_df['loss_class_regr'], 'c')\n",
        "# # plt.plot(np.arange(0, r_epochs), record_df['curr_loss'], 'm')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2oAAAE/CAYAAAA39zBmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd5TU1fnH8fezSxdUFCSKNAVEECysKEYsxALRgIkNFaPGGmvUFGyoqNFoNGrUKComVgQr/kRRjMZuQMFCkyIIqIiCgtLh+f3xzGRnl23A7szOzOd1zp6Z+X6/M3PHnMPN53vvfa65OyIiIiIiIlJ7FGS6ASIiIiIiIlKSgpqIiIiIiEgto6AmIiIiIiJSyyioiYiIiIiI1DIKaiIiIiIiIrWMgpqIiIiIiEgto6AmIiIikoPMbLaZHZTpdgCYmZtZ+0y3QySbKKhJzlHHVL7a9N9GRERERMqnoCYiIiIiOc/M6lTz5xVW5+eJlKagJiIiIpLDzKy+md1qZl8k/m41s/qJc83M7P/M7DszW2Rmb5hZQeLcn8xsvpktNbNpZvazSr6n0MwuNbOZife8b2atyrjuMDObYGZLzGyumV2Vcq6BmT1sZt8m2jTOzFokzp1sZrMSn/2ZmZ1QSXtONrO3zOxvZvYtcFXiv8VfzexzM1tgZnebWcOU9/zRzL5M/Hc6LXVmjJn908z+YWajzexH4MAq/48gshEU1CRn5WvHlHjP6WY2JfGeyWa2RxnX9DCz8Yn2LDCzWyr9jyoiItnoMmBvYDdgV6AHcHni3MXAPKA50AK4FHAz2wk4F9jT3ZsAhwKzK/mei4DjgJ8DmwO/AZaVcd2PwK+BLYHDgN+a2RGJcycBWwCtgK2Bs4DlZrYZcDvQN9GefYCJVfjtewGzEr/tOuAGoGPiv0V7oCUwGMDM+iR+w0GJcweU8XnHJz6nCfBmFb5fZKMpqEkuy8uOycyOBq5KfNfmQD/g2zIuvQ24zd03B3YERlTyO0VEJDudAAxx96/dfSFwNXBi4txqYFugjbuvdvc33N2BtUB9oLOZ1XX32e4+s5LvOQ243N2nefjQ3dfrf9z9NXf/2N3XuftHwGPA/int2Rpo7+5r3f19d1+SOLcO2MXMGrr7l+4+qQq//Qt3/7u7rwFWAGcAF7r7IndfCvwZGJC49hjgAXef5O7LiL60tGfd/a1E21dU4ftFNpqCmuSyfO2YTgNudPdxifbMcPc5ZVy3GmhvZs3c/Qd3f7eSzxURkey0HZDaD8xJHAO4CZgBvJSYvTEIwN1nAL8jwsrXZjbczLajYq2AyvpMzGwvM3vVzBaa2ffEzclmidMPAWOA4YnZMDcm+uMfgWMT135pZs+bWafKfzpzU543BxoB7ydmr3wHvJg4DvHfZG45763omEiNUFCTXJavHVOV2gOcSkz/mJqYanl4Fd4jIiLZ5wugTcrr1oljuPtSd7/Y3XcgZmBclJzy7+6Puvu+ifc68JdKvmcuMUOjMo8Co4BW7r4FcDdgie9c7e5Xu3tnYhbJ4cQMEdx9jLsfTNxonQrcW4Xv8pTn3wDLgS7uvmXibwt3b5w4/yWwfcr16y1jKPV5IjVKQU1yWb52TFVqj7tPd/fjgG2I3/hEYqqliIjklseAy82suZk1I9ZkPQxgZoebWXszM+B7YmbJOjPbycx6J9Z2ryACzrpKvuc+4Boz62Chm5ltXcZ1TYBF7r7CzHoQ675ItOdAM+tqUVFxCTH7Y52ZtTCz/ol+aiXwQxXaU4K7ryP60L+Z2TaJ72tpZocmLhkBnGJmO5tZI+CKDfl8keqmoCa5LF87pvuA35tZ90R72ptZm9IXmdlAM2ue6Li+SxzeoE5PRESywrXAeOAj4GPgg8QxgA7AWKJ/eQe4y91fJZYB3ECMQn1F3NS7pJLvuYUIOy8Rfdn9QMMyrjsbGGJmS4m+OXWN9E+AJxLvnwL8h5h1UkCsCf8CWEQsHfhtVX58KX8iZtS8a2ZLiN++E4C7v0CsC381eU3iPSs34ntENpnFshyR3GFms4l1Wm8CNwJHJ06NBP6YCEoXAhcQ89IXA/e4+zVm1o0IOjsTYelt4Ax3/6KC7yskOq9TiamMU4Ffuvs8M3Ogg7vPMLOjgJuBrYiOZzawpbsPNLPjiOmW2xOd5eNEh9QcGE4URHGikMjZ7j65kv8GZwEXEtWsZgMnuvuE5H8bdx9rZg8DhxDz9ecAl7n7MxV9roiISL4ws52BT4D6iWIkImmloCYiIiIiApjZL4HRxE3MfwHr3P2Iit8lUjM09VFEREREqsTMXjCzH8r4uzRD7bm7nPbcvZEfeSbwNVGUay0bN71SpFpoRE2kCszsBaBXGaf+7O5/zkB77gYGlnHqYXc/K93tEREREZHqpaAmIiIiIiJSy2jqo4iIiIiISC1TJ1Nf3KxZM2/btm2mvl5ERNLo/fff/8bdm2e6HdlCfaSISH6oqH/MWFBr27Yt48ePz9TXi4hIGpnZnEy3IZuojxQRyQ8V9Y+a+igiIiIiIlLLKKiJiIiIiIjUMgpqIiIiIiIitYyCmoiISCXMrI+ZTTOzGWY2qILrjjQzN7OilGOXJN43zcwOTU+LRUQk22WsmIiIiEg2MLNC4E7gYGAeMM7MRrn75FLXNQEuAN5LOdYZGAB0AbYDxppZR3dfm672i4hIdtKImoiISMV6ADPcfZa7rwKGA/3LuO4a4C/AipRj/YHh7r7S3T8DZiQ+T0REpEIKaiIiIhVrCcxNeT0vcex/zGwPoJW7P7+h7xURESmLgpqIiMgmMLMC4Bbg4k38nDPMbLyZjV+4cGH1NE5ERLKWgpqIiEjF5gOtUl5vnziW1ATYBXjNzGYDewOjEgVFKnvv/7j7UHcvcvei5s2bV2PzRUQkG2VvUJs2De65B378MdMtERGR3DYO6GBm7cysHlEcZFTypLt/7+7N3L2tu7cF3gX6ufv4xHUDzKy+mbUDOgD/rfEWjxwJY8fW+NeIiEjNyd6g9vbbcNZZoOkhIiJSg9x9DXAuMAaYAoxw90lmNsTM+lXy3knACGAy8CJwTloqPl55ZdzMFBGRrJW95fkbNYrH5csz2w4REcl57j4aGF3q2OByrj2g1OvrgOtqrHFladgQVqyo/DoREam1sndErWHDeFy2LLPtEBERqW0aNFBQExHJctkb1JIjagpqIiIiJSmoiYhkPQU1ERGRXKOgJiKS9bI3qCWnPmqNmoiISEkKaiIiWS97g5pG1ERERMqmoCYikvUU1ERERHKNgpqISNbL/qCmqY8iIiIlKaiJiGS97A1qKs8vIiJSNgU1EZGsl71BrUGDeFRQExERKUlBTUQk62VvUCsoiFE1BTUREZGSGjSANWviT0REslL2BjWIoKY1aiIiIiUlZ52sXJnZdoiIyEbL7qDWqJFG1EREREpLBjVNfxQRyVoKaiIiIrlGQU1EJOspqImIiOQaBTURkayX3UFNa9RERETWlwxq6iNFRLJWdgc1jaiJiIisTyNqIiJZT0FNREQk1yioiYhkvewOapr6KCIisj4FNRGRrJfdQU0jaiIiIutTUBMRyXoKaiIiIrlGQU1EJOspqImIiOSahg3jUUFNRCRrZXdQS65Rc890S0RERGoPjaiJiGS97A5qjRrFozoiERGRYgpqIiJZLzeCmqY/ioiIFFNQExHJegpqIiIiuUZBTUQk62V3UEsultZeaiIiIsXq1IHCQgU1EZEslt1BTSNqIiKSBmbWx8ymmdkMMxtUxvmzzOxjM5toZm+aWefE8bZmtjxxfKKZ3Z22RjdooKAmIpLF6mS6AZtEQU1ERGqYmRUCdwIHA/OAcWY2yt0np1z2qLvfnbi+H3AL0Cdxbqa775bONgMKaiIiWS67R9Q09VFERGpeD2CGu89y91XAcKB/6gXuviTl5WZA5veNUVATEclq2R3UNKImIiI1ryUwN+X1vMSxEszsHDObCdwInJ9yqp2ZTTCz/5hZr5ptagoFNRGRrKagJiIiUg3c/U533xH4E3B54vCXQGt33x24CHjUzDYv6/1mdoaZjTez8QsXLtz0BimoiYhkNQU1ERGRis0HWqW83j5xrDzDgSMA3H2lu3+beP4+MBPoWNab3H2ouxe5e1Hz5s03vdUKaiIiWS27g5rWqImISM0bB3Qws3ZmVg8YAIxKvcDMOqS8PAyYnjjePFGMBDPbAegAzEpLqxXURESymqo+ioiIVMDd15jZucAYoBAY5u6TzGwIMN7dRwHnmtlBwGpgMXBS4u37AUPMbDWwDjjL3RelpeEKaiIiWa1KQc3M+gC3ER3Ufe5+Q6nzJwM3UTwV5A53v68a21m25IiagpqIiNQgdx8NjC51bHDK8wvKed+TwJM127pyNGgA332Xka8WEZFNV2lQq+L+MQCPu/u5NdDG8tWpA3XrKqiJiIiUphE1EZGsVpU1apXuH5NRjRppjZqIiEhpCmoiIlmtKkGtSvvHAEea2Udm9oSZtSrjfM1o1EgjaiIiIqUpqImIZLXqqvr4HNDW3bsBLwP/Kuuiat8jBhTUREREyqKgJiKS1aoS1CrdP8bdv3X3lYmX9wHdy/qgat8jBqKgiKY+ioiIlNSggfpHEZEsVpWgVpX9Y7ZNedkPmFJ9TayERtRERETWlxxRc890S0REZCNUWvWxivvHnG9m/YA1wCLg5Bpsc0kKaiIiIutr0ADWrYM1a6JCsoiIZJUq7aNWhf1jLgEuqd6mVVHDhvD11xn5ahERkVorudfoihUKaiIiWai6iolkjsrzi4iIrK9Bg3hUQRERkayUG0FNUx9FRERKUlATEclqCmoiIiK5SEFNRCSrZX9Qa9hQQU1ERKQ0BTURkayW/UEtuUatovLD774Lv/99+tokIiKSaQpqIiJZLTeC2tq1sHp1+dc8/TTcfDOsXFn+NSIiIrlEQU1EJKvlRlCDiqc/LlkSj4sX13x7REREagMFNRGRrJb9QS25T0xFJfoV1EREJN8oqImIZLXsD2obMqL23Xc13x4REZHaQEFNRCSr5UdQW7o0HjWiJiIi+UJBTUQkq2V/UEtOfdQaNRERkWIKaiIiWS37g1pyRE1r1ERERIopqImIZLXsD2qbbRaPyTBWFq1RExGRfKOgJiKS1bI/qLVtG4+zZpV/jUbUREQk39SvH48KaiIiWSn7g1rz5rDlljBtWtnnV60q3uhaQU1ERPJFnTrxp6AmIpKVsj+omcFOO8HUqWWfT1Z8BAU1ERHJLw0aKKiJiGSp7A9qAJ06lT+ilrp2TWvUREQkn1QlqM2dW3HlZBERyYjcCGo77QRffll2QZHkMTONqImISH6pSlDbc0+4+eb0tEdERKosd4IawKefrn8uGdS23VZBTURE8ktlQW3dOliwAObNS1+bRESkSnIjqHXqFI9lrVNLrlFr00ZTH0VEZKOYWR8zm2ZmM8xsUBnnzzKzj81sopm9aWadU85dknjfNDM7NK0NryyoJc9VtMWNiIhkRG4EtR13hIKCstepJTufNm3i+dq16W2biIhkNTMrBO4E+gKdgeNSg1jCo+7e1d13A24Ebkm8tzMwAOgC9AHuSnxeejRsWHFQS65N+/779LRHRESqLDeCWv360K5dxUGtdet41KiaiIhsmB7ADHef5e6rgOFA/9QL3D11SGozwBPP+wPD3X2lu38GzEh8Xno0aADLl5d/PhnUNKImIlLr5EZQg1inVpWgpnVqIiKyYVoCc1Nez0scK8HMzjGzmcSI2vkb8t7E+88ws/FmNn7hwoXV0vBKpz4mQ5yCmohIrZM7Qa1Tpygmsm5dyeNLlkTFx1at4rVG1EREpAa4+53uviPwJ+DyjXj/UHcvcvei5s2bV0+jKgtqmvooIlJr5U5Q22mn6Iw+/7zk8aVLoUkT2GqreK0RNRER2TDzgVYpr7dPHCvPcOCIjXxv9WrYEH78sfzzGlETEam1ciuowfrTH5csiaDWtGm8VlATEZENMw7oYGbtzKweURxkVOoFZtYh5eVhwPTE81HAADOrb2btgA7Af9PQ5tCyJcyfD+5ln09do1beNSIikhG5F9RKl+hfsgQ23xy23DJeK6iJiMgGcPc1wLnAGGAKMMLdJ5nZEDPrl7jsXDObZGYTgYuAkxLvnQSMACYDLwLnuHv6yg+3bh0jaosWlX0+GdTWrat45E1ERNKuTqYbUG1atIhRsylTSh5PBrXkiJrWqImIyAZy99HA6FLHBqc8v6CC914HXFdzratAspDW55/D1luvfz61IuSSJdC4cXraJSIilcqdETUz6NwZJk0qeTwZ1Bo2hHr1NKImIiL5o02beJwzp+zzyRE10Do1EZFaJneCGkCXLhHUUufZJ4uJmMWomoKaiIjki9QRtbKUHlETEZFaI7eCWufOEcQWLCg+lhxRg1inpqmPIiKSL5o1ixkl5QW11BE1legXEalVciuodekSj5MnFx9LDWoaURMRkXxiFqNqmvooIpJ1cjOoJdepuSuoiYhIfmvdWlMfRUSyUG4FtZ/8JKY3JkfUfvwxwpqCmoiI5KuKgtqyZTHqBpr6KCJSy+RWUDMrLigCUUgEopgIaI2aiIjknzZt4KuvYMWK9c8tWxbr2EAjaiIitUxuBTUoLtGfnPYIJUfUvvuuZFVIERGRXJas/Dhv3vrnli+Pm5mbbaagJiJSy+ReUOvSBRYtgq+/LjuorV1bPNImIiKS6yoq0b9sGTRqFP2kpj6KiNQqdTLdgGqXWvlx3bp4nhrUINapJY+JiIjksoo2vV6+PILa6tUaURMRqWVyb0Stc+d4nDSpuNNJrlHbZpt4/OKL9LdLREQkE1q2jDXc5Y2oNWwIW2yhoCYiUsvkXlDbdtsoGvLRR8VTHJOjZzvvHI9TpmSmbSIiIulWv35URdbURxGRrJJ7Qc0MeveGp5+GhQvjWDKotW0LDRoUV4UUERHJB23alD/1sWHD6Cc1oiYiUqvkXlADOOMM+OYbeOiheJ0MaoWFMaqW3GdNREQkH5S3l1pyRE1TH0VEap3cDGoHHxx3Dz/8EOrVi2kfScny/SIiIvmiTZsIaskiW0nJYiKa+igiUuvkZlArKIDTT4/nyUIiSV26wNy5unMoIiL5o00bWLkSFiwoeTy1mMjSpesHORERyZjcDGoAp5wSUx1Ll+FPVoVUQREREckX7drF42eflTyeWkzEHX78Mf1tExGRMlUpqJlZHzObZmYzzGxQBdcdaWZuZkXV18SNtN12cNxx0KFDyePJfdY0/VFERPJFWUFt9WpYs6Y4qIGmP4qI1CKVbnhtZoXAncDBwDxgnJmNcvfJpa5rAlwAvFcTDd0oDzwQ0yBTtWsXlR9VUERERPJFctPr2bOLjy1fHo/JqY+gZQEiIrVIVUbUegAz3H2Wu68ChgP9y7juGuAvwIpqbN+mqVNn/aBWWAidOmlETURE8kejRtCiRckRtWRQSx1RU1ATEak1qhLUWgJzU17PSxz7HzPbA2jl7s9XY9tqTpcuGlETEZH80q5dyaC2bFk8JvdRA019FBGpRTa5mIiZFQC3ABdX4dozzGy8mY1fmNyMOhM6d44yxUuXZq4NIiIi6VReUEvuowYaURMRqUWqEtTmA61SXm+fOJbUBNgFeM3MZgN7A6PKKiji7kPdvcjdi5o3b77xrd5UyYIiqvwoIiL5ol27uEm5Zk281tRHEZFarSpBbRzQwczamVk9YAAwKnnS3b9392bu3tbd2wLvAv3cfXyNtLg6dOsWj+PGZbYdIiIi6dKuHaxdC/PmxWtNfRQRqdUqDWruvgY4FxgDTAFGuPskMxtiZv1quoE1ol072HFHePHFTLdEREQkPUqX6E8dUWvSJJ5rRE1EpNaotDw/gLuPBkaXOja4nGsP2PRmpUHfvjBsGKxYEeX6RUREcllqUDvwwJJr1AoKIqwpqImI1BqbXEwka/XpE53UG29kuiUiIlLLmVkfM5tmZjPMbFAZ5y8ys8lm9pGZvWJmbVLOrTWziYm/UaXfmzatWkUgS46opU59hJj+qKmPIiK1Rv4GtQMPhPr14YUXMt0SERGpxcysELgT6At0Bo4zs86lLpsAFLl7N+AJ4MaUc8vdfbfEX+aWDNStC9tvX/bUR4jKjxpRExGpNfI3qDVqBPvvr6AmIiKV6QHMcPdZ7r4KGA70T73A3V9198QQFe8SFZJrn3btYPbseF56RK1pU/jmm4w0S0RE1pe/QQ1indrUqcWdloiIyPpaAnNTXs9LHCvPqUDqXcAGiT1E3zWzI2qigVWWupda6ho1gDZtony/iIjUCgpqoFE1ERGpFmY2ECgCbko53Mbdi4DjgVvNbMdy3ntGItCNX7hwYc00sF07+OKLKKSVnPqYLKiVDGpr19bMd4uIyAbJ76DWsSM0awYTJmS6JSIiUnvNB1qlvN4+cawEMzsIuIzYS3Rl8ri7z088zgJeA3Yv60vcfai7F7l7UfPmzauv9amSlR/nzIkRtYYNwSyOtW0bm2F/8UXNfLeIiGyQ/A5qZrDTTvDpp5luiYiI1F7jgA5m1s7M6gEDgBLVG81sd+AeIqR9nXK8qZnVTzxvBvwUmJy2lpe2Y2Iwb+bMGFFLTnuEGFGDCHEiIpJx+R3UADp0UFATEZFyufsa4FxgDDAFGOHuk8xsiJklqzjeBDQGRpYqw78zMN7MPgReBW5w98wHtRkzYkQtNai1bRuPWrctIlIrVGnD65zWsSP885/www/QuHGmWyMiIrWQu48GRpc6Njjl+UHlvO9toGvNtm4DbLNN9HXJoJas+AjQunU8akRNRKRW0Ihax47xOH16ZtshIiJS08ygffuypz42bAgtWmhETUSkllBQSwY1TX8UEZF80L592SNqEOvUNKImIlIrKKi1bx+PCmoiIpIP2rePvdSWLi05ogaxTk0jaiIitYKCWsOG0KqVgpqIiOSH9u1h9WqYNm39oJYcUVu3LjNtExGR/1FQg5j+qKAmIiL5IDmT5Lvv1p/62LYtrFoFCxakvVkiIlKSghoUBzX3TLdERESkZiVL9EPZI2qg6Y8iIrWAghpEUPvuO/j220y3REREpGZttx00aBDPy1qjBiooIiJSCyiogSo/iohI/igoKB5VK6vqI2hETUSkFlBQA+jQIR4V1EREJB8k16mVHlFr3Bi23lojaiIitYCCGsRUjzp1YPJkWLFC1a5ERCS3JYNa6RE1iFE1jaiJiGScghpA3boxDeSmm6LTOuCATLdIRESk5pQ3ogbaS01EpJZQUEu67z64/no47DB4883YCFRERCQXVRTUdtkllgIsWZLeNomISAkKakn77guDBsHZZ0eZ/gkTMt0iERGRmtG1a4S0HXZY/9y++8YSgHffTX+7RETkfxTUSuvePR7ffz+z7RAREakpLVrA4sXQu/f653r2hMJCeOON9LdLRET+p06mG1DrtGgBLVsqqImISG6rV6/s440bw+67K6iJiGSYRtTK0r27gpqIiOSvffeF996DVasy3RIRkbyloFaW7t1h2jQVFBERkfzUq1dsV6ObliIiGaOgVpbu3aOgyMSJmW6JiIhI+u27bzxq+qOISMYoqJVFBUVERCSfbbMNdOwY29WIiEhGKKiV5Sc/ge22i6C2bh18/nmmWyQiIpJevXpFUFu3LtMtERHJSwpq5eneHV55BfbYA9q00X4yIiKSX3r1ihL+kydnuiUiInlJQa08e+0FX34Jy5ZFCeORIzPdIhERkfRJrlPT9EcRkYxQUCvPBRfAyy/HncTeveHZZ6PAiIiISD7YYQfYdlsVFBERyRAFtfI0bgwHHQR16kD//jBzJkyZkulWiYiIpIdZTH9UUBMRyQgFtar4xS/i8dlnM9sOERGRdNp3X5g7V0W1REQyQEGtKlq2hKIiGDUq0y0RERFJn1694lGjaiIiaaegVlX9+sF778FXX2W6JSIikmZm1sfMppnZDDMbVMb5i8xsspl9ZGavmFmblHMnmdn0xN9J6W35JuraFTbfXEFNRCQDFNSqqn//KCbyxBOZbomIiKSRmRUCdwJ9gc7AcWbWudRlE4Aid+8GPAHcmHjvVsCVwF5AD+BKM2uarrZvssJC2GcfVX4UEckABbWq6toV9twTbr9dm3+KiOSXHsAMd5/l7quA4UD/1Avc/VV3X5Z4+S6wfeL5ocDL7r7I3RcDLwN90tTu6tGrF0yaBN9+m+mWiIjkFQW1qjKDCy+E6dNh9OhMt0ZERNKnJTA35fW8xLHynAq8sJHvrX0OOCAeX3opo80QEck3Cmob4qijYPvt4W9/y3RLRESkFjKzgUARcNNGvPcMMxtvZuMXLlxY/Y3bWHvvHUW1Hn880y0REckrCmobom5dOO88+Pe/YeLETLdGRETSYz7QKuX19oljJZjZQcBlQD93X7kh7wVw96HuXuTuRc2bN6+WhleLggI45hh44QX47rtMt0ZEJG8oqG2o00+HRo3gzjsz3RIREUmPcUAHM2tnZvWAAUCJ/VrMbHfgHiKkfZ1yagxwiJk1TRQROSRxLLsceyysWqX9REVE0khBbUM1bRpTIEeOhBUrMt0aERGpYe6+BjiXCFhTgBHuPsnMhphZv8RlNwGNgZFmNtHMRiXeuwi4hgh744AhiWPZpUcPaNsWhg/PdEtERPJGnUw3ICsNHAgPPgjPPQdHH53p1oiISA1z99HA6FLHBqc8P6iC9w4DhtVc69LADAYMgJtugm++gWbNMt0iEZGcpxG1jdG7N2y3HTz0UKZbIiIikh7HHgtr18aMEhERqXEKahujsBCOPz4WVn/zTaZbIyIiUvN23TX2FH3ggUy3REQkL1QpqJlZHzObZmYzzGxQGefPMrOPE/Py3zSzztXf1FrmxBNhzZqS5Ypfew1++9tYcJ30xRdxB1JERCSbmcGpp8K4cfDxx5lujYhIzqs0qJlZIXAn0BfoDBxXRhB71N27uvtuwI3ALdXe0tqmWzfYbTe48kp48UV45RXo2xfuvjs6MYBFi6B9e7j99sy2VUREpDqccEJsVXP//ZluiYhIzqvKiFoPYIa7z3L3VcBwoH/qBe6+JOXlZoBXXxNrsccfj7VqffvCz38OrVvH8XffjXomCiMAACAASURBVMe33oLly2HEiMy1UUREpLo0awZHHAEPPwwrV1Z+vYiIbLSqBLWWwNyU1/MSx0ows3PMbCYxonZ+9TSvluvYMULZWWfB3nvDm29G+eJ33onzb74Zj++9B199lbFmioiIVJvf/Aa+/RZGjar8WhER2WjVVkzE3e909x2BPwGXl3WNmZ1hZuPNbPzChQur66szq1Ej+Mc/4D//gebNoWfPCGruEdRatIjn//d/mW6piIjIpjv4YGjTBm6+Ofo3ERGpEVUJavOBVimvt08cK89w4IiyTrj7UHcvcvei5s2bV72V2aRnzyggMn16rFU78cQYZdOdRxERyQWFhXDZZTFbZPToyq8XEZGNUpWgNg7oYGbtzKweMAAokTrMrEPKy8OA6dXXxCyz997x+Pe/w+rV0KsX9OsHL78MP/6Y2baJiIhUh5NPhh12gCuu0KiaiEgNqTSoufsa4FxgDDAFGOHuk8xsiJn1S1x2rplNMrOJwEXASTXW4tpu112hQQMYNixe77MP9O8PK1bA2LGZbZuIiEh1qFs3qh5PmABPP53p1oiI5KQqrVFz99Hu3tHdd3T36xLHBrv7qMTzC9y9i7vv5u4Huvukmmx0rVavHhQVwbJlsPPOUSGrVy/YYgutUxMRkdxxwgnQqROcey5Mm5bp1oiI5JxqKyYiKXr2jMd9943HunXj+dtvZ65NIiIi1amwMLafWbMG9t8fJk/OdItERHKKglpNSK5TSwa15LHJk+G77zLTJhERkerWtWtUPS4ogEMPjbXZIiJSLRTUasLhh8Nf/wpHH118LDnK9t57mWmTiIhITdh5Zxg6FObN0xR/EZFqpKBWE+rVg4svhoYNi4/16AFmsUG2iIhILunTB7bdtriQloiIbDIFtXRp0gR22SU2wxYREckldepEyf7Ro2Mv0SVL4JZbYPHiTLdMRCRrKailU8+eMaK2bl2mWyIiIlK9Tjkl+rfbboODD46ZJccfrz5PRGQjKailU8+e8P33MHVqplsiIiJSvTp0gP32gxtvjP3VTjkFXnwRrr020y0TEclKCmrplCwoonVqIiKSiy66CLbcEp55Bu6/H048Ea66Cl55JdMtExHJOgpq6dShAzRtGh1WWSWMJ0wA9/S3S0REpDr07w/ffgs//3kU0Lr77uj7zjwTli/PdOtERLKKglo6FRTEpqCPPgpbbQW/+U3xuUmTYI894L77Mtc+ERGRTVWQ8n8tGjWCf/wDZs6E66/PXJtERLKQglq6PfggjBgR0yAfeCDWrEEENYi7jyIiIrmid28YOBBuuAGmTct0a0REsoaCWro1aRIbYZ99dryePj0eZ8yIxw8+gPffz0zbREREasJf/xqja4MHZ7olIiJZQ0EtUzp0iMdPP43H6dNjOmSjRjB0aObaJSIiUt1atIhRteeeg6VLM90aEZGsoKCWKTvuGAutk0FtxozYEPvYY2MNmzoyERHJJQMGREGR557LdEtERLKCglqmNGgAbdoUT32cPh3at4czzoAffoDHHsts+0RERKrTPvtAy5YwfHimWyIikhUU1DKpY8cYUVu6FBYsiOmQe+0FXbtq+qOIiOSWgoKYNfLii7B4caZbIyJS6ymoZVIyqCULibRvH9MhzzwzCopUpajIypU120YREcHM+pjZNDObYWaDyji/n5l9YGZrzOyoUufWmtnExN+o9LW6FhowIPYRfeaZTLdERKTWU1DLpA4dYMkSeOut4tcAJ5wADRvCvfdW/P5XX40CJA8+WLPtFBHJY2ZWCNwJ9AU6A8eZWedSl30OnAw8WsZHLHf33RJ//Wq0sbVdURHssANceCF06gT9+sV0fxERWY+CWiZ17BiPzz8fjzvuGI9bbhnTQx55pPwO7PPP4ZhjYNkyuOIKWLWq5tsrIpKfegAz3H2Wu68ChgP9Uy9w99nu/hGwLhMNzBpmcMst0KdPTPN//vlYm+2e6ZaJiNQ6CmqZlAxqr74K224LjRsXn0sWFfnnP9d/36JF8KtfRTi79dYIbf/6V1qaLCKSh1oCc1Nez0scq6oGZjbezN41syPKu8jMzkhcN37hwoUb29bar3//KCgyciQMGRLFs+66K9OtEhGpdepkugF5rU0bqFs31pm1b1/y3N57w557wnnnwUsvwUknxbXjxsHtt0eIe+YZOPzw6OSuuy6uqVcvM79FRETK08bd55vZDsC/zexjd59Z+iJ3HwoMBSgqKsqPIaZLLoF33ompkD/8ABdfDHX0f01EREAjaplVWFgc0JLr05LMYOxYuPZaePNNOOqouAt57bVwyCEwcSL84hdx3ZVXwpw58NBD6f8NIiK5bz7QKuX19oljVeLu8xOPs4DXgN2rs3FZraAAHn44bjoOGhSVj+fNy3SrRERqBQW1TEsGtNIjagCbbw6XXRYhLFkFcvbsmC7StWvxdX36xPuffjotTRYRyTPjgA5m1s7M6gEDgCpVbzSzpmZWP/G8GfBTYHKNtTQbbbklPPUUPPEETJ0KZ5+d6RaJiNQKCmqZllynVnpELVWTJrDHHvHXps36582gd2944w1Yu7Zm2ikikqfcfQ1wLjAGmAKMcPdJZjbEzPoBmNmeZjYPOBq4x8wmJd6+MzDezD4EXgVucHcFtbIceSRcfTU89xw8+2ymWyMiknGaCJ5pnTrFYzKwbawDDohNsidOhO7dN7lZIiJSzN1HA6NLHRuc8nwcMSWy9PveBrqWPi7luOCC2HLmvPNi+5rnnoN994Xzz890y0RE0k5BLdOOPz5GzLpuYj++//7x+NprCmoiIpKd6taFu++Gn/4Ufv1rqF8/pvUfeOCm95MiIllGUx8zrWHD2A/NbNM+Z7vtYlTutdeqpVkiIiIZsc8+MGYMvP02zJ0LW2wBv/0trNMWdSKSXxTUcsn++8Prr1d9ndq338Ly5TXbJhERkQ11yCHQsyc0bw433QRvvQX33KONsUUkryio5ZIDDog5/R9+WPm1a9bAbrvBpZfWeLNEREQ22kknwX77RTXITp3gllsU2EQkLyio5ZLUdWqV+c9/Yq+ajz6q0SaJiIhskoKCKCpy112w1VaxKfbUqZlulYhIjVNQyyUtW0aZ/6FDYebMmNZ45plR0n/RopLXjhwZj7Nmpb+dIiIiG2LzzWOd2mOPxesxYzLbHhGRNFBQyzV33glffx17rhUVRWj7/HN4/PHia9asic1FIc6tXp2ZtoqIiGyItm1j+uOLL2a6JSIiNU5BLdccfDBMmACdO8OCBTB6NHTpAg8/XHzN66/DwoVw2GFRRevzzzPXXhERkQ3Rp09M308Ww5oypepFtEREsoiCWi5q0yYqZM2bB337woknRpnjmTPj/MiR0KgRnHNOvNb0RxERyRaHHgorVsRNxzFj4sbk4MGVv09EJMsoqOWqggJo0CCen3BC7NP28MMxkvbkk3D44bDLLnFeQU1ERLLF/vtH//bMM8U3HG+7Lfo3EZEcoqCWD7bfHg48EO69N0ryL1kSndt220G9egpqIiKSPRo2jLB2990xU+Suu2Ia5F/+kumWiYhUKwW1fDFwIMyfD5ttBu+9F3vSFBbGwuzKgtrq1bDTTvDII+ufGzYsFnZrfYCIiKTLoYfG48CBUQ1y4MAopvXFF5ltl4hINVJQyxcnnhhBa/x42HXX4uM77FAc1IYNg5/9LAqMpJozBz79FF54Yf3PffBBmDZNnaOIiKTP8cfDKafAzTfH68GD46bi6afDypVxbO5c+PDDzLVRRGQTKajlizp1omPbfPOSx1OD2j/+Af/+d4y4pZo+PR4nTix5fMmSKFoCmj4pIiLp06JF3FzcZpt4veOOcMcdUen4iCPgb3+L2R49e8L332e2rSIiG0lBLd/tsAN89x188kmMtkHxZthJM2bE49SpxeWQAcaOjT3ZQEFNREQy66yzYi32mDFw0UUxe2T58uJNskVEsoyCWr7bYYd4vO22eOzcGZ54ouT0x+SI2tq18PHHxcdfeCFG6AoLFdRERCTzTjsNnnsubji+9RZ06wb335/pVomIbBQFtXyXDGoPPQTt28OgQTGv/7//Lb5mxgxo1iyeJ6c/ukdQO/hgaN1aQU1ERGqHww6Do46KbWl+85uYLfLRR5lulYjIBlNQy3ft2sXjypXQr1/81asHI0YUXzN9OhxwAGyxBUyYEMc++SSqSPbtW3KdW6rnn4/QJyIikgkDB0afNmxY8bGvvoJHH4VFizLXLhGRKlBQy3ebb148Wta/f4SxQw8tnv64ejXMng0dO8YebMmglqwA2adP2UHt++8j9F19ddp+ioiISAlbbx3FRe6/P/q2vfeOPURPOAEuvTTTrRMRqZCCmkTQ2mor2GefeH3UUTES9sEHUZp/zZqYFrn77jF9ZMUKuO8+2GMPaNky3v/11/DDD8Wf+c47EfReey0jP0lERASAP/4xbjQuWRKja1deCUceGdvLfPttplsnIlKuOlW5yMz6ALcBhcB97n5DqfMXAacBa4CFwG/cfU41t1VqyuWXw7JlUcIfYjqjGbz4IhQVxbEOHaJoyPLlcMEFMR3y+efjXHKd22efQdeu8fzNN+Nx5swIfa1arf+9X30FJ58c3123bpRT7tatxn6miIjkoe7d4Y03Sh6bNAmefBLuuafskbUFC+LcJZdE/yQikgGVjqiZWSFwJ9AX6AwcZ2adS102AShy927AE8CN1d1QqUG/+AUce2zx6+bNo2N74YXiio/t28cdSYChQ2MKSd++8ToZ1FKnP775JjRtGs//85+yv/fll6OM8tq1MfJWelsAERGRmtClSxTDuuMOWLVq/fNDhsTI20svpb9tIiIJVZn62AOY4e6z3H0VMBzon3qBu7/q7ssSL98Ftq/eZkra9e0L774b1R8bN47NRXfeOaaNFBbCzTfHqBusH9RWrYpNs088McJaedMfp06NUbzXXoOddlJVLhERSZ8LL4Qvv4SbboIffyw+vngx/POf8fzZZzPSNBERqFpQawmklu6blzhWnlOBFzalUVIL9O0ba8xGjozRNLOY/nH88TFVskuX4mubNo0iJMmg9sEHsY5t//1hv/3KD2pTpsRn160bG5MqqImISLoki4tcfnnMJLnggpjhce+9MSW/W7fYky11X1ERkTSq1mIiZjYQKAJuKuf8GWY23szGL1y4sDq/Wqpbjx5RYGTlyliflvTAA3DVVSWvNStZ+TG5Pu2nP42y/sl1ar/7XZRKTpoyJUbpIDrE2bOjWqSIiEhNKyiItWuvvBLT/2+/Pfqov/8dDjwwipB89VXJfUUrsnZtfJZ7zbZbRPJGVYLafCC1EsT2iWMlmNlBwGVAP3dfWdYHuftQdy9y96LmzZtvTHslXQoL4ZBD4nn79pVfXzqodegQ0yUPOCCO9ekDt90We9d8/32U/Z8xozio7bprPH78cbX+DBERkXLVqQO9e8dNyL/8BYYPh3nzYlpk377RF44aFbNELrsspvWX5/774aCDVO1YRKpNVYLaOKCDmbUzs3rAAGBU6gVmtjtwDxHSvq7+ZkpG9OkTj1UNap99FuvT3nwT9t03jnfrFlMjJ0+OTs89OroZM6Lsf6dOxdcBfPhh9f+OjTVvnu6MiggQ1Y/NbJqZzTCzQWWc38/MPjCzNWZ2VKlzJ5nZ9MTfSelrtWyQP/4R7r479lg77LCYVbLffvDUU7EX25//HMW35q93rzrcd188vqDVHyJSPSoNau6+BjgXGANMAUa4+yQzG2Jm/RKX3QQ0Bkaa2UQzG1XOx0k2+eUv4aSTiqs7VmSHHWKaZNOmsS/N/vvH8YKC6Nz+/ve4U2kWe6xNmRLnkyNqLVvGezdkndry5TUXpObMid/0r3/VzOeLSNaoYvXjz4GTgUdLvXcr4EpgL6I415Vm1rSm2ywb6cwz4eGHo+8C6N8fpk2L6o9XXhlr1449NmaFpPr4Yxg3LkbgXnwx/e0WkZxUpX3U3H00MLrUscEpzw+q5nZJbbD55sWVrypz0EHQqxfsskvM7f/lL4vPnXVW8fMuXSKoJfelSY6omcX0x+SI2iuvxALugw8u+/u++CI+a++94aGHoFmzDfpplXrppeiIn3469noTkXz2v+rHAGaWrH48OXmBu89OnCtdeeJQ4GV3X5Q4/zLQB3is5pstm+zoo6Mf/P3vY6StUyc47rhYGnDyyRHkttwShg2Lfu3CC+HGG2PUrWVFdddERCpXrcVEJI+1bw+vvw533RUdW51y7gH07BlTHydPjk2wGzcuPtetW9yVnD8/ppkccUTxFJNvvom7nMnqW3/5CyxdCv/+d+zvNn589f6esWPj8ZVXYqRQRPLZhlY/rq73SqZttx1MmBAhDWDAALjllpi+f/LJMfPi/vvjhmH//sXXjRmTsSaLSO5QUJP06tkTvvsORo8unvaYtOuuMa1k4MAIR2vWwKWXxrq3/v1jX7arr459b4YOjWmZ77wTo3Gnn1590yDXrYuA1rJl7K2TrGIpIlKDVBk5S1x4IXz+Obz9NnTuDKedFlP+Tz0VunaFbbdVUBORaqGgJunVs2c8Ll68flBLFhR57bXYz+bCC+HBByOkvf12THMcMgSOOiqmJV52GeyxR6wbmDgR/vOfkp+3ahU89liEuH32iYqTVTFxYnS6V1wRG3xrYbhIvqtS9eNNfa8qI2cRs+jPXn8d7rwzQtrBB8fxPn3g5ZfjZqOIyCZQUJP06tgx5vND8fq0pC5dYgF38+axAemll8I228TC7D/9CV59FfbcM0Lbr38dU04gppo0axbTUZJmz47Kk8cfH5t2f/NNXHf66VHY5OijY01BWZLTHvv1i4pfGxPUFi6M0UERyQWVVj+uwBjgEDNrmigickjimOSCggI4++yo+FhYGMf69ImbkSecEGu0338/s20UkayloCbpVVAQI2Ow/ohaw4YRzu67D7bYIoqZPPggXHwxXHcdNGgQZZJPOy1G1lLfd/bZ8H//B5Mmwb33wu67w6efwuOPx+jY5MkwaFB89vnnw3PPRSnmstafjR0boXHbbaPi5eTJMc2lqlatiu8/44wN/+8jIrVOVaofm9meZjYPOBq4x8wmJd67CLiGCHvjgCHJwiKSow49NGaIvPFGrK3u27f8kv6pFi3SljAiUoJ5hv5RKCoq8vHVXQBCssN118HgwfDVVzF6Vh0WLIDWraOTW706pjo+9FDxqFvS1KkR+KZPj6pdjz0Wi8OTVqyIbQLOPBNuvTW2EejcOfbWOfPMuGbCBPjkk1gzV5aRI+GYY6IC2Pz51fcbRbKYmb3v7kWZbke2UB+ZI6ZOjZkg3brFtP5kxePSXn89qicPGRI3FUUkb1TUP2pETdLvd7+LO43VGWBatIA//CFG655/PgqAlA5pENMt27aFn/0M2rWLoiRJa9dGsZIVK6LDTF7funWsN0i64oooZDJjRtltGTo0NkpdvVr7sImI5LNOnaIq5NtvR1jbe+9Ydz1sWMz2gCiQldyb7YYbYtqkiAgKapIJm20WI17V7dpr467kz38eC7orUlAQ69VefTWmSM6ZEyNsN9xQcpNvswht//53BLnVq+OuqDvcfvv6nztzZkyd/N3v4Kc/jdCmqSwiIvnrmGPgttugTZtYo/3RR1F8pEWL6F/69YMlS6Lg1fffw803l/05S5bETUP1KSJ5Q0FN8tcpp8R+b7/6FXToEKX+hw2LzU2Ti8IhOtLFi+GDD2IPuB9/jD3ghg2LrQb++98IZbfcEoVKCgrgN7+JqZLTp69fjTLV1KmxnuGpp2r854qISIacf34UxnrxRZg1KwqM/PGPMHdu7AN6zz2xkXYy1I0dC5dcEte8806swe7cOW4onn++wppIntAaNclvxx0HTz4ZBUouuSQCWGkLFsBPfgJ//nMUH7nmmthn7cADo/rks8/GaNsPP8T1/frFseXLY7PUvn3L3hrgkUcizP34YxRPmTSpeO+2lStj+mRNWLIk9opLVt8USQOtUdsw6iPzhHvc8GvaNF5PmQK77BL/RtepE7M6Vq+Oc7vsAj16xE3CM86Af/wjbgyKSFbTGjWR8gwbFusD7rqr7JAGMT2lW7e4wzl2LBQVwQEHRFB78EHYeuuoDPn661GO+cor430NG8a6g1GjYt1bqnfeiY29u3ePqZSrVkXH+9ZbsaZht90isG2oqVMjcK5bV/41v/pV7PejO7IiIpllVhzSIKohP/ooPPBA3CRcuDAqR95zT4zC3XdfVEceOrRk9eMRI2Ltd7pMmRL9n/oRkZrl7hn56969u4tkjYsucq9Xz71OHfdLL41j77zjfthh7nPmlP++0aPdIR5T/frX7k2auC9dGq9vvTWuM3Pffvt4fuWVG97Ofv3ivZ98Uvb5+fPjO8D9vfc2/PNFNhIw3jPU32Tjn/pIKde6de4nnRT/lr/wgvu118a/6XXruj/zjPvKle5DhrgffLD7Bx9U//evXeu+227xnRdcEO0RkY1WUf+oETWRqjjooBj1WrOmuCLk3nvHuoHWrct/X+/e0LhxTIVMWrw47n4OHBjnAM47D446Kjbo/uSTGIm78cZYv1BV06fH/nAQ6+bK8uSTcQe0Xr2SFS9FRCQ7mMUskK5d4Ze/hMsvj2n8u+8e/UjXrrEFzrvvxtYAf/xjFMyqLk89BRMnQs+esZ7u3HM1siZSQxTURKqiV6/Y/6Zhw+icqqp+/SgW8txzxdMRH344pkKmbohdUBD7rz38cKxX+8tf4vrTT48NvB9/vPKO8Lbboo2NG5cf1EaOjHUOJ54Ye8gtWVJ8zj22Nlizpuq/T0RE0q9RI3jiCWjSJNZYP/RQVITcZx9Ytiz6nDlzoorxTTfFtjRFRVHZePr0qn9P6X5n7dqY3t+pU0z3P++8CI2ffFKtP09EgoKaSFU0bgx9+sBhh8WG2Ruif3/44otYX+AeI1l77hnr0MrTpk3cER0zJgLdgAEwenTx+alTYenS4teLF8eahuOPh732Kjuoffll7C939NHxmcuWlSxyMmIEHH54rDsQEZHarUOH6FvuvTcqFW++eWw5M3t2/FvetGns4TZjRtz8KyyMNcwdO8Y6t8pMmRL7jT7ySPGx4cNjTfbVV0exk4svjuP//neN/ESRfKegJlJVTz0VndSG+vnPY8TsiSfg1lvjzmPqaFp5Lr0Uvvoq7opuvXXcMYVYYL777nDOOcXXDh0awet3v4uqYB99tH4Bk+S0x6OPjqC4665RNWzt2jie3Ltn5MgN/40iIpJ+deqUfF1QUHJ7GYAdd4zpj++9B59/HjMqrr++4m1hfvgBjjwy+p/zzouiJgsWwB/+EH3HUUfFdW3axOcrqInUCAU1kaqqU2f9DrAqtt4a9t031pxddBHst1+sJ6iKFi1iDdyxx8Y6t++/j3C1YkVMXZw/P6pD3nJL7K+z664R1NasiTUESe5x/S67RFUxs+hwP/oI7rgD3n4bxo2LTnfsWFi0qOJ2uUdbNsRnn0WHnzoSKCIi6dOqVYzA9egBJ58cW81MmBAjc0lr18a0+2nTon/44Yfou447LrYSeOihktsC9O4d+4WuXZv2nyOS6xTURNLh4otjZO3FF6Mc/2abbdj7Bw6McPboo7EeoHv3WMN2551w993w9dfF2wLsuWc8pk5/vPnmCGNnnll87PjjYyrnJZfAn/4U02T+9a8IeanFT1JNnhwjci1aQLNmsQl4VXz2WWxpcMcdMTVHREQyo379mDlRr14Ux9pjj9jDs0cPOOWUeD58OFx7bczc+P3vY/30q69Gf9O1a8nPO/DAuHE3YULJ46NHw4cfpu93ieQgBTWRdOjXLwp1HHpojGZtqL33hvbtYxRs4cIYnevfPzrNG2+MfdH22SeubdkyNtpOBrWxYyOIHXVUyemSZrE3T716sX/bmWfGaF/bttGJu0dgS3a+y5fHHmyvvBKbeDdpAlddVXnb586NkJYcSdOicxGRzGrdOv5tf+aZ+Lv++vg3/8kno3jWyJEwaFBce/nlEeYuugh+/ev1P+vAA+MxdfrjrbfGjcCf/jRG25LcYz30kUfG1H4RqVCdyi8RkYwzi1G1q66K6Y0HHhgB6+mn43xyNC2pR4+Yyjh2LBxzTEx3fOCB9UNiy5YR9gYPjhLLZjFi9re/xebdjz0Wo38vvBBVxKZNi8piBx0Ud1uvuCJG1fbYo/y2//nPMeL39tsR9D7+eOP/O6xeHQF1xx2jymWB7jWJiGyUVq3iD+Lf1WQwK61RIxg/vvybjD/5CXTuHEHtD3+IypKXXgpHHAGffho39q6/PmZtPPVU8YyNb7+NPqr0OjuAmTPjfI8em/47RbJZeRus1fSfNvMU2UCzZrk3bOg+fHi8XrfOvVcv98MPX//a667z/22gvfPO7jNnVvzZqRuW/ve/xe8dNMi9Uyf3Ro3cCwrczzij+LrvvnNv2jQ22S7PypVxzfHHx+vDD3ffZZeq/d6yDBkSbQP3U06JjVclK6ANr9VHSu4699zoJw4+OP59Pu4499Wr3RcscN911+J/txs0cL/pJvcHHojXf/pTyc9Ztcr9+uvd69eP86ee6j59uvu997qfdZb7F19k5OeJ1KSK+keNqIlki3btoshHcnsAs5iGWNZdzt694/HXv451bJWtiUv9jKIiuO662C/uwAPh/PPjceXK2I8naYst4MILYzTu/PPhZz+LqS6pd0dHj46tAwYOjNddu8Y6vVWrYkSwPO4wb17x3V6IwifXXBNbFey0U5SHbtwYbr+94t+W6scf43dstVXV35MrVq6M6afNmmW6JSKSa3r3jjXI77wT66jPPDNmPGyzTYzGff55XLfVVrDllvH83Xdj24B69eCyy2IN9KmnxpTMI4+EHXaIQln331/8Pa+8EiN3228fr3/4IYqcJNc+77BD9Hm9eqXvt4vUpPISXE3/6W6hSA37+uvq+6wVHjKPaQAAGYlJREFUK9y//379499/7/6LX8RdUnD//e9Lnv/Vr9xbtIg7q+7ujzwS1338ccXfd8017nXrus+dG6/XrnXv3t29eXP3hQvj2NlnxyjflCllf8a6de5XX+3+wQfFx0480b1bt8p/by667DL37bbL2CgkGlFTHym5a9Uq99tuc58zp+rvWbEi/k0G97Zt3QsLo7944onia95/P0bgJkxwf+st9803d2/Xzv3ZZ6OP23ff6AfOOsv9ooviHLifc060SSQLVNQ/qhMSkU23fLn7L38Z0xyXLYtjixa516vn/rvfFV/34Yfxz85jj5X/WXPnxhRPcH/wwTg2YUK8vuee4uu+/jqm2pxwQtmfM3FivGfAgHi9Zo37VlvFscWLN/631iZz5rjffnvJqavlOfzw+O2fflrz7SqDgpr6SJEyjR7tvtNOMZ190aKKr/3vf9232cb/Nz2/sNB9xIji8z/84H7BBXG+f/8Ig+7xuRt7k2rZMoU+qVEV9Y9aiS8im65Bg9gjbfHiqBoGUdlr1arYXDWpU6eYGllRQZFLLomtB5o0Ka4WNnZsPB5+ePF1zZtHFcvHHosiJ6UlN+5+6aXY32fixOL94aq6rUBtd/XVMe101qzKr/3ss3h8//2abZOIyIbo2xemToVhw6LgSEX23DMqCb/4Ipx9NowaFQWwkjbbLCpO3nFHFC352c+iIMlWW8V0yXPOKbvy8I8/wiOPxJT7VMuXw+67l+zHSlu3LoptLV9e9d8sUkUKaiJSPQ44ILYQGDo01iNccUV0cLvvXnxNvXrQsWPZHeXy5RGuHn44ykD37h17zkFUmuzcObYdSPX730dIHDSo5Iat7vFZjRpFOEtWwExKhpVPPoEuXdbvnLPB8uXwxBPxvLLw5a6gJiK5oV692Ormjjtif9KynHNO9EXvvhv//g0eHFvY/POf0ScNHhzrdpOuvDLWUrdqFdclbyZef33cCBwxouwbYmvXxt5z/fpFn5fKvVp+ruQ3BTURqR5mcMYZ8MYbcMghsUH3Y4+tX+yka9foBFesiDuhHTtChw6xwPyYY2J/n0suieA3c2b8vfFGbAlQ2jbbRFh75pnYamDXXWH27Pj8Tz+NBeoFBbG9wNix8d2tWxeHlUceiQXszz1X0/91qt9zz8GSJfG8svC1cCEsW1a1a0VEcsHpp8dI2bhxMfvgiSfiJuLxx0dhqp/9LGZ9fPllFCA54ogopDV7dvQ/jz0WWw307QuFhesXrlq9OsLdgw/GljF33hmftWpVvOe3v83Er5Yco6AmItXnpJOgbt24A/nww1GdsbRddonRnVNOiY6zW7eYznLeeRE+Pvkkpj0ecEBcf/31MXpUVlCD2Fvu/ffhr3+FOXNir7YHH4yAdtppsNdeEeTefDM+o6ioOKy88EI8po62bYjZs+G99+IvGYQ21OOPw8UXR/WyDfHQQ//f3plHWVFda/y36RZQUXGeEAETp0SjDCoiRgWFRIVoojjkqYkJS2P0qajROL1n1BdHiD6jMQ4hPnEE5xgkUYmKIA0OEQdsBQEVAREHRKDp/f7YdanbTU/Q0LdKvt9ad/U9p86t+vp0d32965yzTwSne+5Zd/A1b14Ew5COpm27bUz71JNeIcTaQJs2NcubbgrDh8f984UX4kHflVdG0HXttbH/2wsvhAcdd1xMpbzzThg0KKZmFh6OLVkSGYjvvReuvjqm2FdVxbnOOSemZt56a9OmpQvREPUtXlvTLy2UFuIbyg03uN92W/3HH37Yl++pc8kl9berqnJv3z4Wi5eV1Z11sjaPPx4LzMH9wAOjrnjvtSeeSPeYe+MNX76vT/v2cb2V4a67IttY4dynnpoemzAhMmAW9q+rrq5f/267xed33rnxbJgF5sxxLy93P++82NuuffuaCUWqqiLD49lnR/mee+IahUX277xT93lvvNH9qaeapmElQclE5JFCZImzzor7YVmZ+y9+UfPY9OmxT2kh8VVFRbS96KLIJHz44VEeNiz9zODBcS6IbJbl5TWTadXFJ5+49+sXWXlrJ4WqrnafNq1pyaJErmnIH2VCQoiWZdq0uPUcdljjWbgGDIi2++7b9PMXArNbbonyxIlRLi93/+IL97//Pco//Wl8veCC+PrSS+k5qqvdR4yoP2gpBGkHHhgZy3r2rLmR99FHp9fs3999u+2i/PTTNc8zd27UF7Yx2GSTNEtZQ9xwgy/f5uBPf4r3lZXp8XHjoq5HjyhfeWWU//Wv+FrYNL2Yxx6LY5ts0njmtVVAgZo8UohMsWSJe69ekZ14+vTG2/fu7csfzIH7zTfXPD5jRmzU3atXnPv449032CAe0lVWxkbel1/ufvXVse3Axx/HdjGFh4unnZZ64tKlqUd16eJ+7rnu48craPuGokBNCJEt/vnPSKPcGNdf742OvNVm2TL3555LDW/Zskjn3Lt3lAvBUatW7ttuG2YJEcy4h6kOGpQGLcUjYZWV7iedFMZ64IHuCxdG/aWXxvm++CLKHTu6H3yw++mnu++wg/tRR8U/A0OG1NQ6alRc54UX3EeOjPcvvtj497j//jES554+6b3vvvT4xRdHXZs28Q/DL38Ze9AtXhw6zj235vnmz48RuC5d4ns766z02OefN66nCShQk0cKkTkWLqx/hkFtPvrI/aGH4jVpUt1t3nkn9bbCvblfv3TLmeJX69ZR/9RTMQMD4sHefffFtjIQ9+5+/eKhH4Rn7bNPvBqauSJyhQI1IUQ+mTo1RppefbV55xk3zn3KlLTcsWPc/k4+Ocrf+577QQe5f/BBTEEsK4sNVCGmSrq7/+UvUd+2bQQyhSDNPaZcgvvYsXEOcB86tKaG/fePTbuLOeOM2Atu8eIVPzd3bgRjv/pVuvF3ob5VqwjG3GMEbp11Yhpkge7d0yk4L7/s3rdvOrrWrVt8r8WceGK0nzQp/jEoL48pqkcdFaOBixY1uavrQ4GaPFKItY7CKNyAATHitmRJTF2//fZ4IDh2bLSrrnb//e/TPeIgNvouMH+++/DhcU8+5BD373wnfOAf/4jjs2e7P/OMRtxyigI1IYQo5ogj4vb3wANRHjIknm7uuKN7u3ZheO7uhx4ao2pjxsTxgw5y//DDFc9XGJW75hr3Bx+M9+PH12xzySVhrAsWpHW77x4jbwU6doxpk+4RGBZG/lq3TtdKDB8e9RMnpp/r2tW9T5+aWk44Ib7edluM6hXOW3tNWyHIvOiiKM+eHdN1IPriwgvTkcJmkPdADegPvA1UAufXcbwNcF9yfALQKanvBCwCXkletzTlevJIIb4BTJ8e/tHUAKqqyv3ZZ92ffLLhdl984b7LLjFTYtiwuKdDrJ17883wi5NOivv+oEHhdQriMktD/qisj0KItY/vfx/atUszSfbtG1m8PvwQRo9OM05eemnsw9avX2RMfOAB2HrrFc+3xRbQqRO89FLs29O6NeyxR802BxwQG6M+/3yUP/kEXnsttBTo2RNefDHeP/kkbLVVbE+w226RSezrr2OD1222gW7d0s9165Zmcxw9OupOPx023DA0zZgBnTtHfa9esGBBZNNcsCC2VPjud+Gii+L4lltGRrTLLouslpdfHn21FmNmZcBNwA+AXYFjzWzXWs1OBj51928BQ4Grio696+57JK9TWkS0EKL0bL99+EvtbWrqo6wsPKF//4bbtWsHI0dGtuEzzwyPuOyyyGC8yy5w7LHw+OPhMc89F1vhHHlk3NNrM2pU7B83aFDsF7dkSXps3rzwj4aYMSOyZkJ40ODBca2qqoY/U7yPXUszcmTszTphQlr37rvp91EXd98dffvqq2teXzH1RXBr+qWnhUKIkrF0aUw/KbBoUSzkHjduxbaHHRbrCF55peFzHn20e6dOsZC8Z88Vj3/1VYyMnXNOlAvr055/Pm0zbFjUvf+++8YbxxNR95jeAu7XXuu+/voxLbOYW26J44895n7ssTF9Ztky9wMOiLVnxclVqqrcjzsu6nbZJaY8Fo/OrSHI8Yga0BMYXVS+ALigVpvRQM/kfTkwDzBiRO31lb2mPFII0Shjx7r/9a/pmuzKyphC+dxzaSbjpUsjgUnbtnHf32sv96uuirYjRoQH7LRTOu1yt91iRsiNN4bfQMzaGDYspm4WU1ERn+/TJ5YDFNaVw4proQuMGxfT9ffdt/5syNOnxwyV5qyRXrrU/dZbY018MVOmpN/XBhtEQrDBg6P84x/XPfJ4xx1p0pcddnD/9NNV11UHDfmjAjUhhGiIhQsjcGqMa66JW+o666Rp8WvTu3e6VuyMMyIAXLw4PT5+fJzj7LO9RoKQ6mr3/faLc8OK02JmzYpF5hBmcsIJUT9kSGqao0en7YuDtd/+tmn90ExyHqj9BLitqPwfwP/WavM60KGo/C6wWRKoLQReBsYCvZtyTXmkEGK1Mn16JM3q1i31BYj1059/Hr4wcmT6cK+QCOWqq9z33jvKe+wR2xO4R/vu3d032iim6HftGuubf/SjdI33XXe5v/deJGJxjwekHTq4b711tN1rrwg0jzwylgHceGOs0y4EUj171lwuUBfLlq3Y5pVXan6fP/uZ+1tvub/9djyg3GKLyPS8006+fIlBnz7x/rrrap6r8CD0kEPioWl5ufvAgY1nrV4JFKgJIcSaZuzY1BQKa99qc/HFYQjPPOO+1VaR5KOYxYsjU2PbttGuOE3+mDG+fN1YXSn8v/rK/Q9/iG0Cnn026kaMSDVNnVqzfVVVmM7Spav8La8Ma3Gg1gbYNKnrBswENqznOoOBCqCiY8eOq6nnhRCiFtOmRUBy5pkrZmBesMD9N7+JIKt4dGnUqPCtsjL388+Pz0P4TGFk7lvfis9//XUEccUB4e67u++5Z3jc5MnujzySPnzcZptI5FUcIN58cwRFPXpEAFfX67TT4rNt28b+pe4RgLVpE8HY3XfHFjyF5FqFoKwwyvbRR+6//nV8tro6AsayMvf774/Rw8JWOIcemibVGjo06rbfPhKLVVQ0+8fRkD9aHG95unfv7hUVFSW5thBCrHa+/BI22ijWoc2cCR06rNjm6aehT59436EDPPIIdO1as02vXjBuXHwtrGeDsJjDD4eOHeGPf2yapqlTYaedYn3EokXQps2qfW+rATOb5O7dSyagGZhZT+C/3L1fUr4AwN3/p6jN6KTNi2ZWDswGNvdaJmtmzwLnuHuDBiiPFEJkjk8/hSFD4M47o9y3Lzz1VHjM5MmxhruwjvvTT+GJJ2DZMpgzJ/zuxRfhz3+Gn/882rz8cqy93ntvaNUK3nwTPv441umZwWOPxdq5RYvq1rPuurGeb9KkKI8ZAwcfHO8nToz14wBvvBFlgJ13juvVxWefwT77wFtvhZ9/9hkccQTce2+sPYfw4nvugREj4nrnnQe/+92q9ykN+6MCNSGEWF3svnuY08yZdR9ftAi6dIngbPhw2GyzFdsMGQLXXx9JPC68sHl6qquhffswnPo0tRA5D9TKgalAH+ADYCJwnLtPKWpzGrCbu59iZscAR7r70Wa2OTDf3ZeZWRfguaTd/IauKY8UQmSWMWPgppvguutghx2a/rklS9KAp6l89VW86qJdO2jbNoKw/faLYK+QtKtHj5W7ToFFiyL4HDUKNtgAhg6Fddapu+3nn0cCkk03XbVrJTTkj+XNOrMQQoiUK66o/8kfxNO/mTOhvIFbb9++MGwYDBjQfD2tWsWTyRI9kPum4O5VZvZrImFIGXCHu08xs8uIKSuPArcDd5lZJTAfOCb5+P7AZWa2FKgGTmksSBNCiExz8MHpyNXKsLJBGsB668WrIXr0gBtugFNOgZtvXvUgDcKnBw6MV2NsuOGqX6eJaERNCCGyxuzZkZp/dfDll/G1xCn28zyiVgrkkUIIsZLMm1f3TJWMoxE1IYTIE6srSIOSB2hCCCFEi5DDIK0xtOG1EEIIIYQQQmQMBWpCCCGEEEIIkTEUqAkhhBBCCCFExlCgJoQQQgghhBAZo0mBmpn1N7O3zazSzM6v4/j+ZjbZzKrM7CerX6YQQgghhBBCrD00GqiZWRlwE/ADYFfgWDPbtVazGcBJwIjVLVAIIYQQQggh1jaakp5/L6DS3d8DMLN7gYHAG4UG7j49OVa9BjQKIYQQQgghxFpFU6Y+bgvMLCrPSuqEEEIIIYQQQqwBWjSZiJkNNrMKM6uYO3duS15aCCGEEEIIIXJDUwK1D4DtisodkrqVxt1vdffu7t598803X5VTCCGEEEIIIcQ3nqasUZsIfNvMOhMB2jHAcc298KRJk+aZ2fvNPM1mwLzmamlB8qQ3T1ohX3rzpBXypTdPWiFfepurdfvVJWRtYC30yDxphXzpzZNWyJfePGmFfOnNk1Zont56/dHcvdFPm9kPgWFAGXCHu19hZpcBFe7+qJn1AB4CNga+Bma7+3dWUWyTMbMKd+++pq+zusiT3jxphXzpzZNWyJfePGmFfOnNk1YR5OlnlietkC+9edIK+dKbJ62QL7150gprTm9TRtRw978Bf6tVd0nR+4nElEghhBBCCCGEEM2kRZOJCCGEEEIIIYRonLwHareWWsBKkie9edIK+dKbJ62QL7150gr50psnrSLI088sT1ohX3rzpBXypTdPWiFfevOkFdaQ3iatURNCCCGEEEII0XLkfURNCCGEEEIIIb5x5DZQM7P+Zva2mVWa2fml1lOMmW1nZs+Y2RtmNsXM/jOp38TMxpjZO8nXjUuttRgzKzOzl83s8aTc2cwmJH18n5m1LrVGADNrb2YPmtlbZvammfXMct+a2VnJ78HrZnaPmbXNUt+a2R1mNsfMXi+qq7M/Lbgh0f2amXXNgNZrkt+F18zsITNrX3TsgkTr22bWryW11qe36NgQM3Mz2ywpZ65vk/rTk/6dYmZXF9WXtG9F/WTZHyGfHpkXf4R8eaT8sUX0ZtIj8+SPDeld4x7p7rl7EdsEvAt0AVoDrwK7llpXkb6tga7J+w2AqcCuwNXA+Un9+cBVpdZaS/fZwAjg8aR8P3BM8v4W4NRSa0y0DAd+kbxvDbTPat8C2wLTgHWL+vSkLPUtsD/QFXi9qK7O/gR+CDwJGLAPMCEDWg8BypP3VxVp3TW5N7QBOif3jLJS603qtwNGA+8Dm2W4bw8E/gG0ScpbZKVv9ar355hpf0w05s4j8+KPiZ5ceKT8scX0ZtIj8+SPDfTtGvfIvI6o7QVUuvt77r4EuBcYWGJNy3H3j9x9cvL+C+BN4oY0kLiBknz9UWkUroiZdQAOBW5LygYcBDyYNMmEXjPbiPhjuR3A3Ze4+wIy3LfENhjrmlk5sB7wERnqW3f/FzC/VnV9/TkQ+KsH44H2ZrZ1yyitW6u7P+XuVUlxPOlWIQOBe919sbtPAyqJe0eLUU/fAgwFzgOKFwlnrm+BU4Hfu/vipM2cIq0l7VtRL5n2R8ifR+bFHyGXHil/XI3kySPz5I9QOo/Ma6C2LTCzqDwrqcscZtYJ2BOYAGzp7h8lh2YDW5ZIVl0MI/4wqpPypsCCoj/urPRxZ2AucGcyDeU2M1ufjPatu38AXAvMIAzoM2AS2ezbYurrz6z/7f2ceOoGGdVqZgOBD9z91VqHsqh3R6B3Mg1prJn1SOqzqFUEufrZ5MQj8+KPkCOPlD+WhEx7ZM78EVrAI/MaqOUCM2sHjATOdPfPi495jI1mIuWmmR0GzHH3SaXW0gTKiaHnm919T2AhMfVgORnr242JJyudgW2A9YH+JRW1kmSpPxvCzC4EqoC7S62lPsxsPeC3wCWl1tJEyoFNiKkm5wL3J6MJQjSbPHhkzvwRcuSR8seWJesemUN/hBbwyLwGah8Qc1gLdEjqMoOZrUMY0N3uPiqp/rgwVJt8nVPf51uYXsAAM5tOTJM5CPgDMbRcnrTJSh/PAma5+4Sk/CBhSlnt277ANHef6+5LgVFEf2exb4uprz8z+bdnZicBhwHHJ8YJ2dS6A/FPyavJ31sHYLKZbUU29c4CRiXTTV4iRhQ2I5taRZCLn02OPDJP/gj58kj5YwuRE4/Mmz9CC3hkXgO1icC3LTIDtQaOAR4tsablJNH07cCb7n590aFHgROT9ycCj7S0trpw9wvcvYO7dyL68ml3Px54BvhJ0iwTet19NjDTzHZKqvoAb5DRviWmdOxjZuslvxcFvZnr21rU15+PAickGZj2AT4rmgJSEsysPzEtaYC7f1V06FHgGDNrY2adgW8DL5VCYwF3/7e7b+HunZK/t1lEUoXZZLBvgYeJxdKY2Y5EYoJ5ZLBvxXIy7Y+QL4/Mkz9C7jxS/tgC5MUjc+iP0BIe6S2cNWV1vYgMMFOJTCoXllpPLW37EUPhrwGvJK8fEvPa/wm8Q2SJ2aTUWuvQfgBpVqsuyS9WJfAASVabUr+APYCKpH8fBjbOct8C/w28BbwO3EVkAcpM3wL3EOsDlhI3xpPr608i49JNyd/dv4HuGdBaScwFL/yt3VLU/sJE69vAD7LQt7WOTyfNapXFvm0N/F/yuzsZOCgrfatXgz/LzPpjoi+XHpkHf0y05cYj5Y8tojeTHpknf2ygb9e4R1pyMiGEEEIIIYQQGSGvUx+FEEIIIYQQ4huLAjUhhBBCCCGEyBgK1IQQQgghhBAiYyhQE0IIIYQQQoiMoUBNCCGEEEIIITKGAjUhhBBCCCGEyBgK1IQQQgghhBAiYyhQE0IIIYQQQoiM8f+WeEs3U4N9HQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fgI0ovv3zlr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}